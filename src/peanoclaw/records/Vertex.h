#ifndef _PEANOCLAW_RECORDS_VERTEX_H
#define _PEANOCLAW_RECORDS_VERTEX_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>
#include "peano/utils/Globals.h"

namespace peanoclaw {
   namespace records {
      class Vertex;
      class VertexPacked;
   }
}

#if defined(Parallel) && defined(Asserts)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 12-04-2013 09:18
    *
    * @date   31/07/2013 16:41
    */
   class peanoclaw::records::Vertex { 
      
      public:
         
         typedef peanoclaw::records::VertexPacked Packed;
         
         enum InsideOutsideDomain {
            Inside = 0, Boundary = 1, Outside = 2
         };
         
         enum RefinementControl {
            Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
         };
         
         struct PersistentRecords {
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
            #endif
            #ifdef UseManualAlignment
            std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto;
            #endif
            bool _shouldRefine;
            bool _isHangingNode;
            RefinementControl _refinementControl;
            int _adjacentCellsHeight;
            InsideOutsideDomain _insideOutsideDomain;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _x;
            #endif
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
            #endif
            bool _adjacentSubtreeForksIntoOtherRank;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _indicesOfAdjacentCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentSubcellsEraseVeto;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
            }
            
            
            
            inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _shouldRefine;
            }
            
            
            
            inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _shouldRefine = shouldRefine;
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isHangingNode;
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isHangingNode = isHangingNode;
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementControl;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementControl = refinementControl;
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _insideOutsideDomain;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _insideOutsideDomain = insideOutsideDomain;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _x = (x);
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentSubtreeForksIntoOtherRank;
            }
            
            
            
            inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         int _adjacentCellsHeightOfPreviousIteration;
         int _numberOfAdjacentRefinedCells;
         
      public:
         /**
          * Generated
          */
         Vertex();
         
         /**
          * Generated
          */
         Vertex(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
         
         /**
          * Generated
          */
         Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
         
         /**
          * Generated
          */
         virtual ~Vertex();
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._indicesOfAdjacentCellDescriptions;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
         }
         
         
         
         inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
            
         }
         
         
         
         inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._adjacentSubcellsEraseVeto;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
         }
         
         
         
         inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            return _persistentRecords._adjacentSubcellsEraseVeto[elementIndex];
            
         }
         
         
         
         inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            _persistentRecords._adjacentSubcellsEraseVeto[elementIndex]= adjacentSubcellsEraseVeto;
            
         }
         
         
         
         inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            _persistentRecords._adjacentSubcellsEraseVeto.flip(elementIndex);
         }
         
         
         
         inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._shouldRefine;
         }
         
         
         
         inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._shouldRefine = shouldRefine;
         }
         
         
         
         inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isHangingNode;
         }
         
         
         
         inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isHangingNode = isHangingNode;
         }
         
         
         
         inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementControl;
         }
         
         
         
         inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementControl = refinementControl;
         }
         
         
         
         inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._adjacentCellsHeight;
         }
         
         
         
         inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
         }
         
         
         
         inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _adjacentCellsHeightOfPreviousIteration;
         }
         
         
         
         inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
         }
         
         
         
         inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _numberOfAdjacentRefinedCells;
         }
         
         
         
         inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
         }
         
         
         
         inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._insideOutsideDomain;
         }
         
         
         
         inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._insideOutsideDomain = insideOutsideDomain;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._x;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._x = (x);
         }
         
         
         
         inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._x[elementIndex];
            
         }
         
         
         
         inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._x[elementIndex]= x;
            
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._adjacentRanks;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._adjacentRanks = (adjacentRanks);
         }
         
         
         
         inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            return _persistentRecords._adjacentRanks[elementIndex];
            
         }
         
         
         
         inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
            
         }
         
         
         
         inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._adjacentSubtreeForksIntoOtherRank;
         }
         
         
         
         inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const InsideOutsideDomain& param);
         
         /**
          * Generated
          */
         static std::string getInsideOutsideDomainMapping();
         
         /**
          * Generated
          */
         static std::string toString(const RefinementControl& param);
         
         /**
          * Generated
          */
         static std::string getRefinementControlMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         VertexPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
            int _senderDestinationRank;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            int getSenderRank() const;
            
      #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 12-04-2013 09:18
       *
       * @date   31/07/2013 16:41
       */
      class peanoclaw::records::VertexPacked { 
         
         public:
            
            typedef peanoclaw::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef peanoclaw::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
               int _adjacentCellsHeight;
               tarch::la::Vector<DIMENSIONS,double> _x;
               int _level;
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  adjacentSubcellsEraseVeto	| startbit 0	| #bits TWO_POWER_D
                |  shouldRefine	| startbit TWO_POWER_D + 0	| #bits 1
                |  isHangingNode	| startbit TWO_POWER_D + 1	| #bits 1
                |  refinementControl	| startbit TWO_POWER_D + 2	| #bits 3
                |  insideOutsideDomain	| startbit TWO_POWER_D + 5	| #bits 2
                |  adjacentSubtreeForksIntoOtherRank	| startbit TWO_POWER_D + 7	| #bits 1
                */
               short int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _indicesOfAdjacentCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                  mask = static_cast<short int>(mask << (0));
                  short int tmp = static_cast<short int>(_packedRecords0 & mask);
                  tmp = static_cast<short int>(tmp >> (0));
                  std::bitset<TWO_POWER_D> result = tmp;
                  return result;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                  mask = static_cast<short int>(mask << (0));
                  _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                  _packedRecords0 = static_cast<short int>(_packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
               }
               
               
               
               inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (TWO_POWER_D);
   _packedRecords0 = static_cast<short int>( shouldRefine ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (TWO_POWER_D + 1);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (TWO_POWER_D + 2));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
               inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (TWO_POWER_D + 7);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (TWO_POWER_D + 7);
   _packedRecords0 = static_cast<short int>( adjacentSubtreeForksIntoOtherRank ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
            };
            
         private: 
            PersistentRecords _persistentRecords;
            int _adjacentCellsHeightOfPreviousIteration;
            int _numberOfAdjacentRefinedCells;
            
         public:
            /**
             * Generated
             */
            VertexPacked();
            
            /**
             * Generated
             */
            VertexPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
            
            /**
             * Generated
             */
            VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
            
            /**
             * Generated
             */
            virtual ~VertexPacked();
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._indicesOfAdjacentCellDescriptions;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
            }
            
            
            
            inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
               
            }
            
            
            
            inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
               mask = static_cast<short int>(mask << (0));
               short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
               tmp = static_cast<short int>(tmp >> (0));
               std::bitset<TWO_POWER_D> result = tmp;
               return result;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
               mask = static_cast<short int>(mask << (0));
               _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
               _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
            }
            
            
            
            inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               int mask = 1 << (0);
               mask = mask << elementIndex;
               return (_persistentRecords._packedRecords0& mask);
            }
            
            
            
            inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               assertion(!adjacentSubcellsEraseVeto || adjacentSubcellsEraseVeto==1);
               int shift        = 0 + elementIndex; 
               int mask         = 1     << (shift);
               int shiftedValue = adjacentSubcellsEraseVeto << (shift);
               _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
               _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
            }
            
            
            
            inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               int mask = 1 << (0);
               mask = mask << elementIndex;
               _persistentRecords._packedRecords0^= mask;
            }
            
            
            
            inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (TWO_POWER_D);
   _persistentRecords._packedRecords0 = static_cast<short int>( shouldRefine ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (TWO_POWER_D + 1);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (TWO_POWER_D + 2));
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfAdjacentRefinedCells;
            }
            
            
            
            inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._x = (x);
            }
            
            
            
            inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._x[elementIndex];
               
            }
            
            
            
            inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._x[elementIndex]= x;
               
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._adjacentRanks[elementIndex];
               
            }
            
            
            
            inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
               
            }
            
            
            
            inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (TWO_POWER_D + 7);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (TWO_POWER_D + 7);
   _persistentRecords._packedRecords0 = static_cast<short int>( adjacentSubtreeForksIntoOtherRank ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const InsideOutsideDomain& param);
            
            /**
             * Generated
             */
            static std::string getInsideOutsideDomainMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementControl& param);
            
            /**
             * Generated
             */
            static std::string getRefinementControlMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            Vertex convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               
         #endif
            
         };
         
         #ifdef PackedRecords
         #pragma pack (pop)
         #endif
         
         
         
      #elif !defined(Parallel) && !defined(Asserts)
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 12-04-2013 09:18
          *
          * @date   31/07/2013 16:41
          */
         class peanoclaw::records::Vertex { 
            
            public:
               
               typedef peanoclaw::records::VertexPacked Packed;
               
               enum InsideOutsideDomain {
                  Inside = 0, Boundary = 1, Outside = 2
               };
               
               enum RefinementControl {
                  Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
               };
               
               struct PersistentRecords {
                  #ifdef UseManualAlignment
                  tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
                  #endif
                  #ifdef UseManualAlignment
                  std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto __attribute__((aligned(VectorisationAlignment)));
                  #else
                  std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto;
                  #endif
                  bool _shouldRefine;
                  bool _isHangingNode;
                  RefinementControl _refinementControl;
                  int _adjacentCellsHeight;
                  InsideOutsideDomain _insideOutsideDomain;
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _indicesOfAdjacentCellDescriptions;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentSubcellsEraseVeto;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
                  }
                  
                  
                  
                  inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _shouldRefine;
                  }
                  
                  
                  
                  inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _shouldRefine = shouldRefine;
                  }
                  
                  
                  
                  inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isHangingNode;
                  }
                  
                  
                  
                  inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isHangingNode = isHangingNode;
                  }
                  
                  
                  
                  inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _refinementControl;
                  }
                  
                  
                  
                  inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _refinementControl = refinementControl;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentCellsHeight;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentCellsHeight = adjacentCellsHeight;
                  }
                  
                  
                  
                  inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _insideOutsideDomain;
                  }
                  
                  
                  
                  inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _insideOutsideDomain = insideOutsideDomain;
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._indicesOfAdjacentCellDescriptions;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
               }
               
               
               
               inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
                  
               }
               
               
               
               inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentSubcellsEraseVeto;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
               }
               
               
               
               inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  return _persistentRecords._adjacentSubcellsEraseVeto[elementIndex];
                  
               }
               
               
               
               inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentSubcellsEraseVeto[elementIndex]= adjacentSubcellsEraseVeto;
                  
               }
               
               
               
               inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<TWO_POWER_D);
                  _persistentRecords._adjacentSubcellsEraseVeto.flip(elementIndex);
               }
               
               
               
               inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._shouldRefine;
               }
               
               
               
               inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._shouldRefine = shouldRefine;
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 12-04-2013 09:18
             *
             * @date   31/07/2013 16:41
             */
            class peanoclaw::records::VertexPacked { 
               
               public:
                  
                  typedef peanoclaw::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
                  
                  typedef peanoclaw::records::Vertex::RefinementControl RefinementControl;
                  
                  struct PersistentRecords {
                     tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
                     int _adjacentCellsHeight;
                     
                     /** mapping of records:
                     || Member 	|| startbit 	|| length
                      |  adjacentSubcellsEraseVeto	| startbit 0	| #bits TWO_POWER_D
                      |  shouldRefine	| startbit TWO_POWER_D + 0	| #bits 1
                      |  isHangingNode	| startbit TWO_POWER_D + 1	| #bits 1
                      |  refinementControl	| startbit TWO_POWER_D + 2	| #bits 3
                      |  insideOutsideDomain	| startbit TWO_POWER_D + 5	| #bits 2
                      */
                     short int _packedRecords0;
                     
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _indicesOfAdjacentCellDescriptions;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                        mask = static_cast<short int>(mask << (0));
                        short int tmp = static_cast<short int>(_packedRecords0 & mask);
                        tmp = static_cast<short int>(tmp >> (0));
                        std::bitset<TWO_POWER_D> result = tmp;
                        return result;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                        mask = static_cast<short int>(mask << (0));
                        _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                        _packedRecords0 = static_cast<short int>(_packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
                     }
                     
                     
                     
                     inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D);
   _packedRecords0 = static_cast<short int>( shouldRefine ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D + 1);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (TWO_POWER_D + 2));
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  int _adjacentCellsHeightOfPreviousIteration;
                  int _numberOfAdjacentRefinedCells;
                  
               public:
                  /**
                   * Generated
                   */
                  VertexPacked();
                  
                  /**
                   * Generated
                   */
                  VertexPacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
                  
                  /**
                   * Generated
                   */
                  VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain);
                  
                  /**
                   * Generated
                   */
                  virtual ~VertexPacked();
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._indicesOfAdjacentCellDescriptions;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                  }
                  
                  
                  
                  inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                     mask = static_cast<short int>(mask << (0));
                     short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                     tmp = static_cast<short int>(tmp >> (0));
                     std::bitset<TWO_POWER_D> result = tmp;
                     return result;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                     mask = static_cast<short int>(mask << (0));
                     _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                     _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
                  }
                  
                  
                  
                  inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     int mask = 1 << (0);
                     mask = mask << elementIndex;
                     return (_persistentRecords._packedRecords0& mask);
                  }
                  
                  
                  
                  inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     assertion(!adjacentSubcellsEraseVeto || adjacentSubcellsEraseVeto==1);
                     int shift        = 0 + elementIndex; 
                     int mask         = 1     << (shift);
                     int shiftedValue = adjacentSubcellsEraseVeto << (shift);
                     _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                     _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                  }
                  
                  
                  
                  inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     int mask = 1 << (0);
                     mask = mask << elementIndex;
                     _persistentRecords._packedRecords0^= mask;
                  }
                  
                  
                  
                  inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (TWO_POWER_D);
   _persistentRecords._packedRecords0 = static_cast<short int>( shouldRefine ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (TWO_POWER_D + 1);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                  }
                  
                  
                  
                  inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (TWO_POWER_D + 2));
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._adjacentCellsHeight;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                  }
                  
                  
                  
                  inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const InsideOutsideDomain& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getInsideOutsideDomainMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const RefinementControl& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getRefinementControlMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  Vertex convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifdef PackedRecords
               #pragma pack (pop)
               #endif
               
               
               
            
         #elif !defined(Parallel) && defined(Asserts)
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 12-04-2013 09:18
             *
             * @date   31/07/2013 16:41
             */
            class peanoclaw::records::Vertex { 
               
               public:
                  
                  typedef peanoclaw::records::VertexPacked Packed;
                  
                  enum InsideOutsideDomain {
                     Inside = 0, Boundary = 1, Outside = 2
                  };
                  
                  enum RefinementControl {
                     Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
                  };
                  
                  struct PersistentRecords {
                     #ifdef UseManualAlignment
                     tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
                     #endif
                     #ifdef UseManualAlignment
                     std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto __attribute__((aligned(VectorisationAlignment)));
                     #else
                     std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto;
                     #endif
                     bool _shouldRefine;
                     bool _isHangingNode;
                     RefinementControl _refinementControl;
                     int _adjacentCellsHeight;
                     InsideOutsideDomain _insideOutsideDomain;
                     #ifdef UseManualAlignment
                     tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<DIMENSIONS,double> _x;
                     #endif
                     int _level;
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _indicesOfAdjacentCellDescriptions;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentSubcellsEraseVeto;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
                     }
                     
                     
                     
                     inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _shouldRefine;
                     }
                     
                     
                     
                     inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _shouldRefine = shouldRefine;
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isHangingNode;
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isHangingNode = isHangingNode;
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _refinementControl;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _refinementControl = refinementControl;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _insideOutsideDomain;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _insideOutsideDomain = insideOutsideDomain;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _x;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _x = (x);
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _level = level;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  int _adjacentCellsHeightOfPreviousIteration;
                  int _numberOfAdjacentRefinedCells;
                  
               public:
                  /**
                   * Generated
                   */
                  Vertex();
                  
                  /**
                   * Generated
                   */
                  Vertex(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                  
                  /**
                   * Generated
                   */
                  Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                  
                  /**
                   * Generated
                   */
                  virtual ~Vertex();
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._indicesOfAdjacentCellDescriptions;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                  }
                  
                  
                  
                  inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._adjacentSubcellsEraseVeto;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
                  }
                  
                  
                  
                  inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     return _persistentRecords._adjacentSubcellsEraseVeto[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     _persistentRecords._adjacentSubcellsEraseVeto[elementIndex]= adjacentSubcellsEraseVeto;
                     
                  }
                  
                  
                  
                  inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<TWO_POWER_D);
                     _persistentRecords._adjacentSubcellsEraseVeto.flip(elementIndex);
                  }
                  
                  
                  
                  inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._shouldRefine;
                  }
                  
                  
                  
                  inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._shouldRefine = shouldRefine;
                  }
                  
                  
                  
                  inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isHangingNode;
                  }
                  
                  
                  
                  inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isHangingNode = isHangingNode;
                  }
                  
                  
                  
                  inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._refinementControl;
                  }
                  
                  
                  
                  inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._refinementControl = refinementControl;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._adjacentCellsHeight;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._insideOutsideDomain;
                  }
                  
                  
                  
                  inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._insideOutsideDomain = insideOutsideDomain;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._x;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._x = (x);
                  }
                  
                  
                  
                  inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._x[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._x[elementIndex]= x;
                     
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._level = level;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const InsideOutsideDomain& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getInsideOutsideDomainMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const RefinementControl& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getRefinementControlMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  VertexPacked convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifndef DaStGenPackedPadding
                 #define DaStGenPackedPadding 1      // 32 bit version
                 // #define DaStGenPackedPadding 2   // 64 bit version
               #endif
               
               
               #ifdef PackedRecords
                  #pragma pack (push, DaStGenPackedPadding)
               #endif
               
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 12-04-2013 09:18
                *
                * @date   31/07/2013 16:41
                */
               class peanoclaw::records::VertexPacked { 
                  
                  public:
                     
                     typedef peanoclaw::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
                     
                     typedef peanoclaw::records::Vertex::RefinementControl RefinementControl;
                     
                     struct PersistentRecords {
                        tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
                        int _adjacentCellsHeight;
                        tarch::la::Vector<DIMENSIONS,double> _x;
                        int _level;
                        
                        /** mapping of records:
                        || Member 	|| startbit 	|| length
                         |  adjacentSubcellsEraseVeto	| startbit 0	| #bits TWO_POWER_D
                         |  shouldRefine	| startbit TWO_POWER_D + 0	| #bits 1
                         |  isHangingNode	| startbit TWO_POWER_D + 1	| #bits 1
                         |  refinementControl	| startbit TWO_POWER_D + 2	| #bits 3
                         |  insideOutsideDomain	| startbit TWO_POWER_D + 5	| #bits 2
                         */
                        short int _packedRecords0;
                        
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _indicesOfAdjacentCellDescriptions;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                           mask = static_cast<short int>(mask << (0));
                           short int tmp = static_cast<short int>(_packedRecords0 & mask);
                           tmp = static_cast<short int>(tmp >> (0));
                           std::bitset<TWO_POWER_D> result = tmp;
                           return result;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                           mask = static_cast<short int>(mask << (0));
                           _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                           _packedRecords0 = static_cast<short int>(_packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
                        }
                        
                        
                        
                        inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D);
   _packedRecords0 = static_cast<short int>( shouldRefine ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D + 1);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                        }
                        
                        
                        
                        inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (TWO_POWER_D + 2));
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentCellsHeight;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentCellsHeight = adjacentCellsHeight;
                        }
                        
                        
                        
                        inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                        }
                        
                        
                        
                        inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _x;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _x = (x);
                        }
                        
                        
                        
                        inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _level;
                        }
                        
                        
                        
                        inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _level = level;
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     int _adjacentCellsHeightOfPreviousIteration;
                     int _numberOfAdjacentRefinedCells;
                     
                  public:
                     /**
                      * Generated
                      */
                     VertexPacked();
                     
                     /**
                      * Generated
                      */
                     VertexPacked(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                     
                     /**
                      * Generated
                      */
                     VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                     
                     /**
                      * Generated
                      */
                     virtual ~VertexPacked();
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._indicesOfAdjacentCellDescriptions;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                     }
                     
                     
                     
                     inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
                        
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                        mask = static_cast<short int>(mask << (0));
                        short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                        tmp = static_cast<short int>(tmp >> (0));
                        std::bitset<TWO_POWER_D> result = tmp;
                        return result;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                        mask = static_cast<short int>(mask << (0));
                        _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                        _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
                     }
                     
                     
                     
                     inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        int mask = 1 << (0);
                        mask = mask << elementIndex;
                        return (_persistentRecords._packedRecords0& mask);
                     }
                     
                     
                     
                     inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        assertion(!adjacentSubcellsEraseVeto || adjacentSubcellsEraseVeto==1);
                        int shift        = 0 + elementIndex; 
                        int mask         = 1     << (shift);
                        int shiftedValue = adjacentSubcellsEraseVeto << (shift);
                        _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                        _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                     }
                     
                     
                     
                     inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        int mask = 1 << (0);
                        mask = mask << elementIndex;
                        _persistentRecords._packedRecords0^= mask;
                     }
                     
                     
                     
                     inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D);
   _persistentRecords._packedRecords0 = static_cast<short int>( shouldRefine ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (TWO_POWER_D + 1);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (TWO_POWER_D + 2));
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._x;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._x = (x);
                     }
                     
                     
                     
                     inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        return _persistentRecords._x[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._x[elementIndex]= x;
                        
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._level = level;
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const InsideOutsideDomain& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getInsideOutsideDomainMapping();
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const RefinementControl& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getRefinementControlMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     Vertex convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifdef PackedRecords
                  #pragma pack (pop)
                  #endif
                  
                  
                  
               
            #elif defined(Parallel) && !defined(Asserts)
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 12-04-2013 09:18
                *
                * @date   31/07/2013 16:41
                */
               class peanoclaw::records::Vertex { 
                  
                  public:
                     
                     typedef peanoclaw::records::VertexPacked Packed;
                     
                     enum InsideOutsideDomain {
                        Inside = 0, Boundary = 1, Outside = 2
                     };
                     
                     enum RefinementControl {
                        Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
                     };
                     
                     struct PersistentRecords {
                        #ifdef UseManualAlignment
                        tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
                        #endif
                        #ifdef UseManualAlignment
                        std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto __attribute__((aligned(VectorisationAlignment)));
                        #else
                        std::bitset<TWO_POWER_D> _adjacentSubcellsEraseVeto;
                        #endif
                        bool _shouldRefine;
                        bool _isHangingNode;
                        RefinementControl _refinementControl;
                        int _adjacentCellsHeight;
                        InsideOutsideDomain _insideOutsideDomain;
                        #ifdef UseManualAlignment
                        tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
                        #endif
                        bool _adjacentSubtreeForksIntoOtherRank;
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _indicesOfAdjacentCellDescriptions;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentSubcellsEraseVeto;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
                        }
                        
                        
                        
                        inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _shouldRefine;
                        }
                        
                        
                        
                        inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _shouldRefine = shouldRefine;
                        }
                        
                        
                        
                        inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _isHangingNode;
                        }
                        
                        
                        
                        inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _isHangingNode = isHangingNode;
                        }
                        
                        
                        
                        inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _refinementControl;
                        }
                        
                        
                        
                        inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _refinementControl = refinementControl;
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentCellsHeight;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentCellsHeight = adjacentCellsHeight;
                        }
                        
                        
                        
                        inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _insideOutsideDomain;
                        }
                        
                        
                        
                        inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _insideOutsideDomain = insideOutsideDomain;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentRanks;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentRanks = (adjacentRanks);
                        }
                        
                        
                        
                        inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentSubtreeForksIntoOtherRank;
                        }
                        
                        
                        
                        inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     int _adjacentCellsHeightOfPreviousIteration;
                     int _numberOfAdjacentRefinedCells;
                     
                  public:
                     /**
                      * Generated
                      */
                     Vertex();
                     
                     /**
                      * Generated
                      */
                     Vertex(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
                     
                     /**
                      * Generated
                      */
                     Vertex(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
                     
                     /**
                      * Generated
                      */
                     virtual ~Vertex();
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._indicesOfAdjacentCellDescriptions;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                     }
                     
                     
                     
                     inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
                        
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentSubcellsEraseVeto;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentSubcellsEraseVeto = (adjacentSubcellsEraseVeto);
                     }
                     
                     
                     
                     inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        return _persistentRecords._adjacentSubcellsEraseVeto[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        _persistentRecords._adjacentSubcellsEraseVeto[elementIndex]= adjacentSubcellsEraseVeto;
                        
                     }
                     
                     
                     
                     inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        _persistentRecords._adjacentSubcellsEraseVeto.flip(elementIndex);
                     }
                     
                     
                     
                     inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._shouldRefine;
                     }
                     
                     
                     
                     inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._shouldRefine = shouldRefine;
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._isHangingNode;
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._isHangingNode = isHangingNode;
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._refinementControl;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._refinementControl = refinementControl;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._insideOutsideDomain;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._insideOutsideDomain = insideOutsideDomain;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentRanks;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentRanks = (adjacentRanks);
                     }
                     
                     
                     
                     inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        return _persistentRecords._adjacentRanks[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                        
                     }
                     
                     
                     
                     inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentSubtreeForksIntoOtherRank;
                     }
                     
                     
                     
                     inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentSubtreeForksIntoOtherRank = adjacentSubtreeForksIntoOtherRank;
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const InsideOutsideDomain& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getInsideOutsideDomainMapping();
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const RefinementControl& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getRefinementControlMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     VertexPacked convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifndef DaStGenPackedPadding
                    #define DaStGenPackedPadding 1      // 32 bit version
                    // #define DaStGenPackedPadding 2   // 64 bit version
                  #endif
                  
                  
                  #ifdef PackedRecords
                     #pragma pack (push, DaStGenPackedPadding)
                  #endif
                  
                  /**
                   * @author This class is generated by DaStGen
                   * 		   DataStructureGenerator (DaStGen)
                   * 		   2007-2009 Wolfgang Eckhardt
                   * 		   2012      Tobias Weinzierl
                   *
                   * 		   build date: 12-04-2013 09:18
                   *
                   * @date   31/07/2013 16:41
                   */
                  class peanoclaw::records::VertexPacked { 
                     
                     public:
                        
                        typedef peanoclaw::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
                        
                        typedef peanoclaw::records::Vertex::RefinementControl RefinementControl;
                        
                        struct PersistentRecords {
                           tarch::la::Vector<TWO_POWER_D,int> _indicesOfAdjacentCellDescriptions;
                           int _adjacentCellsHeight;
                           tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
                           
                           /** mapping of records:
                           || Member 	|| startbit 	|| length
                            |  adjacentSubcellsEraseVeto	| startbit 0	| #bits TWO_POWER_D
                            |  shouldRefine	| startbit TWO_POWER_D + 0	| #bits 1
                            |  isHangingNode	| startbit TWO_POWER_D + 1	| #bits 1
                            |  refinementControl	| startbit TWO_POWER_D + 2	| #bits 3
                            |  insideOutsideDomain	| startbit TWO_POWER_D + 5	| #bits 2
                            |  adjacentSubtreeForksIntoOtherRank	| startbit TWO_POWER_D + 7	| #bits 1
                            */
                           short int _packedRecords0;
                           
                           /**
                            * Generated
                            */
                           PersistentRecords();
                           
                           /**
                            * Generated
                            */
                           PersistentRecords(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _indicesOfAdjacentCellDescriptions;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                              mask = static_cast<short int>(mask << (0));
                              short int tmp = static_cast<short int>(_packedRecords0 & mask);
                              tmp = static_cast<short int>(tmp >> (0));
                              std::bitset<TWO_POWER_D> result = tmp;
                              return result;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                              mask = static_cast<short int>(mask << (0));
                              _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                              _packedRecords0 = static_cast<short int>(_packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
                           }
                           
                           
                           
                           inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (TWO_POWER_D);
   _packedRecords0 = static_cast<short int>( shouldRefine ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (TWO_POWER_D + 1);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                           }
                           
                           
                           
                           inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (TWO_POWER_D + 2));
                           }
                           
                           
                           
                           inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _adjacentCellsHeight;
                           }
                           
                           
                           
                           inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _adjacentCellsHeight = adjacentCellsHeight;
                           }
                           
                           
                           
                           inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                           }
                           
                           
                           
                           inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _adjacentRanks;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _adjacentRanks = (adjacentRanks);
                           }
                           
                           
                           
                           inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (TWO_POWER_D + 7);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (TWO_POWER_D + 7);
   _packedRecords0 = static_cast<short int>( adjacentSubtreeForksIntoOtherRank ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                        };
                        
                     private: 
                        PersistentRecords _persistentRecords;
                        int _adjacentCellsHeightOfPreviousIteration;
                        int _numberOfAdjacentRefinedCells;
                        
                     public:
                        /**
                         * Generated
                         */
                        VertexPacked();
                        
                        /**
                         * Generated
                         */
                        VertexPacked(const PersistentRecords& persistentRecords);
                        
                        /**
                         * Generated
                         */
                        VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
                        
                        /**
                         * Generated
                         */
                        VertexPacked(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions, const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto, const bool& shouldRefine, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks, const bool& adjacentSubtreeForksIntoOtherRank);
                        
                        /**
                         * Generated
                         */
                        virtual ~VertexPacked();
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getIndicesOfAdjacentCellDescriptions() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._indicesOfAdjacentCellDescriptions;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setIndicesOfAdjacentCellDescriptions(const tarch::la::Vector<TWO_POWER_D,int>& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._indicesOfAdjacentCellDescriptions = (indicesOfAdjacentCellDescriptions);
                        }
                        
                        
                        
                        inline int getIndicesOfAdjacentCellDescriptions(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           return _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setIndicesOfAdjacentCellDescriptions(int elementIndex, const int& indicesOfAdjacentCellDescriptions) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           _persistentRecords._indicesOfAdjacentCellDescriptions[elementIndex]= indicesOfAdjacentCellDescriptions;
                           
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<TWO_POWER_D> getAdjacentSubcellsEraseVeto() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                           mask = static_cast<short int>(mask << (0));
                           short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                           tmp = static_cast<short int>(tmp >> (0));
                           std::bitset<TWO_POWER_D> result = tmp;
                           return result;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentSubcellsEraseVeto(const std::bitset<TWO_POWER_D>& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (TWO_POWER_D)) - 1 ;
                           mask = static_cast<short int>(mask << (0));
                           _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                           _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | adjacentSubcellsEraseVeto.to_ulong() << (0));
                        }
                        
                        
                        
                        inline bool getAdjacentSubcellsEraseVeto(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           int mask = 1 << (0);
                           mask = mask << elementIndex;
                           return (_persistentRecords._packedRecords0& mask);
                        }
                        
                        
                        
                        inline void setAdjacentSubcellsEraseVeto(int elementIndex, const bool& adjacentSubcellsEraseVeto) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           assertion(!adjacentSubcellsEraseVeto || adjacentSubcellsEraseVeto==1);
                           int shift        = 0 + elementIndex; 
                           int mask         = 1     << (shift);
                           int shiftedValue = adjacentSubcellsEraseVeto << (shift);
                           _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                           _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                        }
                        
                        
                        
                        inline void flipAdjacentSubcellsEraseVeto(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           int mask = 1 << (0);
                           mask = mask << elementIndex;
                           _persistentRecords._packedRecords0^= mask;
                        }
                        
                        
                        
                        inline bool getShouldRefine() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setShouldRefine(const bool& shouldRefine) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D);
   _persistentRecords._packedRecords0 = static_cast<short int>( shouldRefine ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D + 1);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D + 1);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 2));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                        }
                        
                        
                        
                        inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 2));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (TWO_POWER_D + 2));
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._adjacentCellsHeight;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentCellsHeightOfPreviousIteration;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                        }
                        
                        
                        
                        inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _numberOfAdjacentRefinedCells;
                        }
                        
                        
                        
                        inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                        }
                        
                        
                        
                        inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (TWO_POWER_D + 5));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                        }
                        
                        
                        
                        inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (TWO_POWER_D + 5));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (TWO_POWER_D + 5));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._adjacentRanks;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._adjacentRanks = (adjacentRanks);
                        }
                        
                        
                        
                        inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           return _persistentRecords._adjacentRanks[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                           
                        }
                        
                        
                        
                        inline bool getAdjacentSubtreeForksIntoOtherRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D + 7);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setAdjacentSubtreeForksIntoOtherRank(const bool& adjacentSubtreeForksIntoOtherRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (TWO_POWER_D + 7);
   _persistentRecords._packedRecords0 = static_cast<short int>( adjacentSubtreeForksIntoOtherRank ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const InsideOutsideDomain& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getInsideOutsideDomainMapping();
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const RefinementControl& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getRefinementControlMapping();
                        
                        /**
                         * Generated
                         */
                        std::string toString() const;
                        
                        /**
                         * Generated
                         */
                        void toString(std::ostream& out) const;
                        
                        
                        PersistentRecords getPersistentRecords() const;
                        /**
                         * Generated
                         */
                        Vertex convert() const;
                        
                        
                     #ifdef Parallel
                        protected:
                           static tarch::logging::Log _log;
                           
                           int _senderDestinationRank;
                           
                        public:
                           
                           /**
                            * Global that represents the mpi datatype.
                            * There are two variants: Datatype identifies only those attributes marked with
                            * parallelise. FullDatatype instead identifies the whole record with all fields.
                            */
                           static MPI_Datatype Datatype;
                           static MPI_Datatype FullDatatype;
                           
                           /**
                            * Initializes the data type for the mpi operations. Has to be called
                            * before the very first send or receive operation is called.
                            */
                           static void initDatatype();
                           
                           static void shutdownDatatype();
                           
                           void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           int getSenderRank() const;
                           
                     #endif
                        
                     };
                     
                     #ifdef PackedRecords
                     #pragma pack (pop)
                     #endif
                     
                     
                     
                  
               #endif
               
               #endif
               
