#ifndef _PEANOCLAW_RECORDS_CELL_H
#define _PEANOCLAW_RECORDS_CELL_H

#include "tarch/multicore/MulticoreDefinitions.h"
#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace peanoclaw {
   namespace records {
      class Cell;
      class CellPacked;
   }
}

#if !defined(Debug) && !defined(Parallel) && defined(SharedMemoryParallelisation)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 12-04-2013 09:18
    *
    * @date   31/07/2013 16:41
    */
   class peanoclaw::records::Cell { 
      
      public:
         
         typedef peanoclaw::records::CellPacked Packed;
         
         enum State {
            Leaf = 0, Refined = 1, Root = 2
         };
         
         struct PersistentRecords {
            int _cellDescriptionIndex;
            bool _isInside;
            State _state;
            #ifdef UseManualAlignment
            std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
            #else
            std::bitset<DIMENSIONS> _evenFlags;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
            #endif
            int _numberOfLoadsFromInputStream;
            int _numberOfStoresToOutputStream;
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
            
            
            inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _cellDescriptionIndex;
            }
            
            
            
            inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _cellDescriptionIndex = cellDescriptionIndex;
            }
            
            
            
            inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isInside;
            }
            
            
            
            inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isInside = isInside;
            }
            
            
            
            inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _state;
            }
            
            
            
            inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _state = state;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _evenFlags;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _evenFlags = (evenFlags);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _accessNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _accessNumber = (accessNumber);
            }
            
            
            
            inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfLoadsFromInputStream;
            }
            
            
            
            inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
            }
            
            
            
            inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfStoresToOutputStream;
            }
            
            
            
            inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         
      public:
         /**
          * Generated
          */
         Cell();
         
         /**
          * Generated
          */
         Cell(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
         
         /**
          * Generated
          */
         virtual ~Cell();
         
         
         inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._cellDescriptionIndex;
         }
         
         
         
         inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
         }
         
         
         
         inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isInside;
         }
         
         
         
         inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isInside = isInside;
         }
         
         
         
         inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._state;
         }
         
         
         
         inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._state = state;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._evenFlags;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._evenFlags = (evenFlags);
         }
         
         
         
         inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._evenFlags[elementIndex];
            
         }
         
         
         
         inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._evenFlags[elementIndex]= evenFlags;
            
         }
         
         
         
         inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._evenFlags.flip(elementIndex);
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._accessNumber;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._accessNumber = (accessNumber);
         }
         
         
         
         inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            return _persistentRecords._accessNumber[elementIndex];
            
         }
         
         
         
         inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS_TIMES_TWO);
            _persistentRecords._accessNumber[elementIndex]= accessNumber;
            
         }
         
         
         
         inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfLoadsFromInputStream;
         }
         
         
         
         inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
         }
         
         
         
         inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._numberOfStoresToOutputStream;
         }
         
         
         
         inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const State& param);
         
         /**
          * Generated
          */
         static std::string getStateMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         CellPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
            int _senderDestinationRank;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            int getSenderRank() const;
            
      #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 12-04-2013 09:18
       *
       * @date   31/07/2013 16:41
       */
      class peanoclaw::records::CellPacked { 
         
         public:
            
            typedef peanoclaw::records::Cell::State State;
            
            struct PersistentRecords {
               int _cellDescriptionIndex;
               tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
               int _numberOfLoadsFromInputStream;
               int _numberOfStoresToOutputStream;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isInside	| startbit 0	| #bits 1
                |  state	| startbit 1	| #bits 2
                |  evenFlags	| startbit 3	| #bits DIMENSIONS
                */
               short int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
               
               
               inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _cellDescriptionIndex;
               }
               
               
               
               inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _cellDescriptionIndex = cellDescriptionIndex;
               }
               
               
               
               inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
               }
               
               
               
               inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                  mask = static_cast<short int>(mask << (3));
                  short int tmp = static_cast<short int>(_packedRecords0 & mask);
                  tmp = static_cast<short int>(tmp >> (3));
                  std::bitset<DIMENSIONS> result = tmp;
                  return result;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                  mask = static_cast<short int>(mask << (3));
                  _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                  _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _accessNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _accessNumber = (accessNumber);
               }
               
               
               
               inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfLoadsFromInputStream;
               }
               
               
               
               inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
               }
               
               
               
               inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfStoresToOutputStream;
               }
               
               
               
               inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
               }
               
               
               
            };
            
         private: 
            PersistentRecords _persistentRecords;
            
         public:
            /**
             * Generated
             */
            CellPacked();
            
            /**
             * Generated
             */
            CellPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
            
            /**
             * Generated
             */
            virtual ~CellPacked();
            
            
            inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._cellDescriptionIndex;
            }
            
            
            
            inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
            }
            
            
            
            inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
            }
            
            
            
            inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
               mask = static_cast<short int>(mask << (3));
               short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
               tmp = static_cast<short int>(tmp >> (3));
               std::bitset<DIMENSIONS> result = tmp;
               return result;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
               mask = static_cast<short int>(mask << (3));
               _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
               _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
            }
            
            
            
            inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               int mask = 1 << (3);
               mask = mask << elementIndex;
               return (_persistentRecords._packedRecords0& mask);
            }
            
            
            
            inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               assertion(!evenFlags || evenFlags==1);
               int shift        = 3 + elementIndex; 
               int mask         = 1     << (shift);
               int shiftedValue = evenFlags << (shift);
               _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
               _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
            }
            
            
            
            inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               int mask = 1 << (3);
               mask = mask << elementIndex;
               _persistentRecords._packedRecords0^= mask;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._accessNumber;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._accessNumber = (accessNumber);
            }
            
            
            
            inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               return _persistentRecords._accessNumber[elementIndex];
               
            }
            
            
            
            inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS_TIMES_TWO);
               _persistentRecords._accessNumber[elementIndex]= accessNumber;
               
            }
            
            
            
            inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfLoadsFromInputStream;
            }
            
            
            
            inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
            }
            
            
            
            inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._numberOfStoresToOutputStream;
            }
            
            
            
            inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const State& param);
            
            /**
             * Generated
             */
            static std::string getStateMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            Cell convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               
         #endif
            
         };
         
         #ifdef PackedRecords
         #pragma pack (pop)
         #endif
         
         
         
      #elif !defined(Parallel) && defined(Debug) && !defined(SharedMemoryParallelisation)
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 12-04-2013 09:18
          *
          * @date   31/07/2013 16:41
          */
         class peanoclaw::records::Cell { 
            
            public:
               
               typedef peanoclaw::records::CellPacked Packed;
               
               enum State {
                  Leaf = 0, Refined = 1, Root = 2
               };
               
               struct PersistentRecords {
                  int _cellDescriptionIndex;
                  bool _isInside;
                  State _state;
                  int _level;
                  #ifdef UseManualAlignment
                  std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                  #else
                  std::bitset<DIMENSIONS> _evenFlags;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                  #endif
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                  
                  
                  inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _cellDescriptionIndex;
                  }
                  
                  
                  
                  inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _cellDescriptionIndex = cellDescriptionIndex;
                  }
                  
                  
                  
                  inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isInside;
                  }
                  
                  
                  
                  inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isInside = isInside;
                  }
                  
                  
                  
                  inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _state;
                  }
                  
                  
                  
                  inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _state = state;
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _evenFlags;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _evenFlags = (evenFlags);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _accessNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _accessNumber = (accessNumber);
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               
            public:
               /**
                * Generated
                */
               Cell();
               
               /**
                * Generated
                */
               Cell(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
               
               /**
                * Generated
                */
               virtual ~Cell();
               
               
               inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._cellDescriptionIndex;
               }
               
               
               
               inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
               }
               
               
               
               inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isInside;
               }
               
               
               
               inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isInside = isInside;
               }
               
               
               
               inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._state;
               }
               
               
               
               inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._state = state;
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._evenFlags;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._evenFlags = (evenFlags);
               }
               
               
               
               inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._evenFlags[elementIndex];
                  
               }
               
               
               
               inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._evenFlags[elementIndex]= evenFlags;
                  
               }
               
               
               
               inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._evenFlags.flip(elementIndex);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._accessNumber;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._accessNumber = (accessNumber);
               }
               
               
               
               inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  return _persistentRecords._accessNumber[elementIndex];
                  
               }
               
               
               
               inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                  _persistentRecords._accessNumber[elementIndex]= accessNumber;
                  
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const State& param);
               
               /**
                * Generated
                */
               static std::string getStateMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               CellPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 12-04-2013 09:18
             *
             * @date   31/07/2013 16:41
             */
            class peanoclaw::records::CellPacked { 
               
               public:
                  
                  typedef peanoclaw::records::Cell::State State;
                  
                  struct PersistentRecords {
                     int _cellDescriptionIndex;
                     int _level;
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                     
                     /** mapping of records:
                     || Member 	|| startbit 	|| length
                      |  isInside	| startbit 0	| #bits 1
                      |  state	| startbit 1	| #bits 2
                      |  evenFlags	| startbit 3	| #bits DIMENSIONS
                      */
                     short int _packedRecords0;
                     
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                     
                     
                     inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _cellDescriptionIndex;
                     }
                     
                     
                     
                     inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _cellDescriptionIndex = cellDescriptionIndex;
                     }
                     
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _level = level;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        short int tmp = static_cast<short int>(_packedRecords0 & mask);
                        tmp = static_cast<short int>(tmp >> (3));
                        std::bitset<DIMENSIONS> result = tmp;
                        return result;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                        _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _accessNumber = (accessNumber);
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  
               public:
                  /**
                   * Generated
                   */
                  CellPacked();
                  
                  /**
                   * Generated
                   */
                  CellPacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                  
                  /**
                   * Generated
                   */
                  virtual ~CellPacked();
                  
                  
                  inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._cellDescriptionIndex;
                  }
                  
                  
                  
                  inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                  }
                  
                  
                  
                  inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                  }
                  
                  
                  
                  inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._level = level;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                     mask = static_cast<short int>(mask << (3));
                     short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                     tmp = static_cast<short int>(tmp >> (3));
                     std::bitset<DIMENSIONS> result = tmp;
                     return result;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                     mask = static_cast<short int>(mask << (3));
                     _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                     _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                  }
                  
                  
                  
                  inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     int mask = 1 << (3);
                     mask = mask << elementIndex;
                     return (_persistentRecords._packedRecords0& mask);
                  }
                  
                  
                  
                  inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     assertion(!evenFlags || evenFlags==1);
                     int shift        = 3 + elementIndex; 
                     int mask         = 1     << (shift);
                     int shiftedValue = evenFlags << (shift);
                     _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                     _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                  }
                  
                  
                  
                  inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     int mask = 1 << (3);
                     mask = mask << elementIndex;
                     _persistentRecords._packedRecords0^= mask;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._accessNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._accessNumber = (accessNumber);
                  }
                  
                  
                  
                  inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._accessNumber[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._accessNumber[elementIndex]= accessNumber;
                     
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const State& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getStateMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  Cell convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifdef PackedRecords
               #pragma pack (pop)
               #endif
               
               
               
            
         #elif defined(Parallel) && !defined(Debug) && !defined(SharedMemoryParallelisation)
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 12-04-2013 09:18
             *
             * @date   31/07/2013 16:41
             */
            class peanoclaw::records::Cell { 
               
               public:
                  
                  typedef peanoclaw::records::CellPacked Packed;
                  
                  enum State {
                     Leaf = 0, Refined = 1, Root = 2
                  };
                  
                  struct PersistentRecords {
                     int _cellDescriptionIndex;
                     bool _isInside;
                     State _state;
                     #ifdef UseManualAlignment
                     std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                     #else
                     std::bitset<DIMENSIONS> _evenFlags;
                     #endif
                     #ifdef UseManualAlignment
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                     #endif
                     int _responsibleRank;
                     bool _subtreeHoldsWorker;
                     double _nodeWorkload;
                     double _localWorkload;
                     double _totalWorkload;
                     bool _cellIsAForkCandidate;
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                     
                     
                     inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _cellDescriptionIndex;
                     }
                     
                     
                     
                     inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _cellDescriptionIndex = cellDescriptionIndex;
                     }
                     
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isInside;
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isInside = isInside;
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _state;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _state = state;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _evenFlags;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _evenFlags = (evenFlags);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _responsibleRank;
                     }
                     
                     
                     
                     inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _responsibleRank = responsibleRank;
                     }
                     
                     
                     
                     inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _subtreeHoldsWorker = subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _nodeWorkload;
                     }
                     
                     
                     
                     inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _nodeWorkload = nodeWorkload;
                     }
                     
                     
                     
                     inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _localWorkload;
                     }
                     
                     
                     
                     inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _localWorkload = localWorkload;
                     }
                     
                     
                     
                     inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _totalWorkload;
                     }
                     
                     
                     
                     inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _totalWorkload = totalWorkload;
                     }
                     
                     
                     
                     inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _cellIsAForkCandidate;
                     }
                     
                     
                     
                     inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _cellIsAForkCandidate = cellIsAForkCandidate;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  
               public:
                  /**
                   * Generated
                   */
                  Cell();
                  
                  /**
                   * Generated
                   */
                  Cell(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                  
                  /**
                   * Generated
                   */
                  virtual ~Cell();
                  
                  
                  inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._cellDescriptionIndex;
                  }
                  
                  
                  
                  inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                  }
                  
                  
                  
                  inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isInside;
                  }
                  
                  
                  
                  inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isInside = isInside;
                  }
                  
                  
                  
                  inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._state;
                  }
                  
                  
                  
                  inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._state = state;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._evenFlags;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._evenFlags = (evenFlags);
                  }
                  
                  
                  
                  inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._evenFlags[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._evenFlags[elementIndex]= evenFlags;
                     
                  }
                  
                  
                  
                  inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._evenFlags.flip(elementIndex);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._accessNumber;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._accessNumber = (accessNumber);
                  }
                  
                  
                  
                  inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     return _persistentRecords._accessNumber[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                     _persistentRecords._accessNumber[elementIndex]= accessNumber;
                     
                  }
                  
                  
                  
                  inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._responsibleRank;
                  }
                  
                  
                  
                  inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._responsibleRank = responsibleRank;
                  }
                  
                  
                  
                  inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._subtreeHoldsWorker;
                  }
                  
                  
                  
                  inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                  }
                  
                  
                  
                  inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._nodeWorkload;
                  }
                  
                  
                  
                  inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._nodeWorkload = nodeWorkload;
                  }
                  
                  
                  
                  inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._localWorkload;
                  }
                  
                  
                  
                  inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._localWorkload = localWorkload;
                  }
                  
                  
                  
                  inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._totalWorkload;
                  }
                  
                  
                  
                  inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._totalWorkload = totalWorkload;
                  }
                  
                  
                  
                  inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._cellIsAForkCandidate;
                  }
                  
                  
                  
                  inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const State& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getStateMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  CellPacked convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifndef DaStGenPackedPadding
                 #define DaStGenPackedPadding 1      // 32 bit version
                 // #define DaStGenPackedPadding 2   // 64 bit version
               #endif
               
               
               #ifdef PackedRecords
                  #pragma pack (push, DaStGenPackedPadding)
               #endif
               
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 12-04-2013 09:18
                *
                * @date   31/07/2013 16:41
                */
               class peanoclaw::records::CellPacked { 
                  
                  public:
                     
                     typedef peanoclaw::records::Cell::State State;
                     
                     struct PersistentRecords {
                        int _cellDescriptionIndex;
                        tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                        int _responsibleRank;
                        bool _subtreeHoldsWorker;
                        double _nodeWorkload;
                        double _localWorkload;
                        double _totalWorkload;
                        
                        /** mapping of records:
                        || Member 	|| startbit 	|| length
                         |  isInside	| startbit 0	| #bits 1
                         |  state	| startbit 1	| #bits 2
                         |  evenFlags	| startbit 3	| #bits DIMENSIONS
                         |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                         */
                        short int _packedRecords0;
                        
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                        
                        
                        inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _cellDescriptionIndex;
                        }
                        
                        
                        
                        inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _cellDescriptionIndex = cellDescriptionIndex;
                        }
                        
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           short int tmp = static_cast<short int>(_packedRecords0 & mask);
                           tmp = static_cast<short int>(tmp >> (3));
                           std::bitset<DIMENSIONS> result = tmp;
                           return result;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                           _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _responsibleRank;
                        }
                        
                        
                        
                        inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _responsibleRank = responsibleRank;
                        }
                        
                        
                        
                        inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _subtreeHoldsWorker = subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _nodeWorkload;
                        }
                        
                        
                        
                        inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _nodeWorkload = nodeWorkload;
                        }
                        
                        
                        
                        inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _localWorkload;
                        }
                        
                        
                        
                        inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _localWorkload = localWorkload;
                        }
                        
                        
                        
                        inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _totalWorkload;
                        }
                        
                        
                        
                        inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _totalWorkload = totalWorkload;
                        }
                        
                        
                        
                        inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     
                  public:
                     /**
                      * Generated
                      */
                     CellPacked();
                     
                     /**
                      * Generated
                      */
                     CellPacked(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                     
                     /**
                      * Generated
                      */
                     virtual ~CellPacked();
                     
                     
                     inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._cellDescriptionIndex;
                     }
                     
                     
                     
                     inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                     }
                     
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                        tmp = static_cast<short int>(tmp >> (3));
                        std::bitset<DIMENSIONS> result = tmp;
                        return result;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                        mask = static_cast<short int>(mask << (3));
                        _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                        _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                     }
                     
                     
                     
                     inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        int mask = 1 << (3);
                        mask = mask << elementIndex;
                        return (_persistentRecords._packedRecords0& mask);
                     }
                     
                     
                     
                     inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        assertion(!evenFlags || evenFlags==1);
                        int shift        = 3 + elementIndex; 
                        int mask         = 1     << (shift);
                        int shiftedValue = evenFlags << (shift);
                        _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                        _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                     }
                     
                     
                     
                     inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        int mask = 1 << (3);
                        mask = mask << elementIndex;
                        _persistentRecords._packedRecords0^= mask;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        return _persistentRecords._accessNumber[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        _persistentRecords._accessNumber[elementIndex]= accessNumber;
                        
                     }
                     
                     
                     
                     inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._responsibleRank;
                     }
                     
                     
                     
                     inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._responsibleRank = responsibleRank;
                     }
                     
                     
                     
                     inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                     }
                     
                     
                     
                     inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._nodeWorkload;
                     }
                     
                     
                     
                     inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._nodeWorkload = nodeWorkload;
                     }
                     
                     
                     
                     inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._localWorkload;
                     }
                     
                     
                     
                     inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._localWorkload = localWorkload;
                     }
                     
                     
                     
                     inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._totalWorkload;
                     }
                     
                     
                     
                     inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._totalWorkload = totalWorkload;
                     }
                     
                     
                     
                     inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const State& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getStateMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     Cell convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifdef PackedRecords
                  #pragma pack (pop)
                  #endif
                  
                  
                  
               
            #elif !defined(Debug) && !defined(Parallel) && !defined(SharedMemoryParallelisation)
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 12-04-2013 09:18
                *
                * @date   31/07/2013 16:41
                */
               class peanoclaw::records::Cell { 
                  
                  public:
                     
                     typedef peanoclaw::records::CellPacked Packed;
                     
                     enum State {
                        Leaf = 0, Refined = 1, Root = 2
                     };
                     
                     struct PersistentRecords {
                        int _cellDescriptionIndex;
                        bool _isInside;
                        State _state;
                        #ifdef UseManualAlignment
                        std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                        #else
                        std::bitset<DIMENSIONS> _evenFlags;
                        #endif
                        #ifdef UseManualAlignment
                        tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                        #endif
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                        
                        
                        inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _cellDescriptionIndex;
                        }
                        
                        
                        
                        inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _cellDescriptionIndex = cellDescriptionIndex;
                        }
                        
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _isInside;
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _isInside = isInside;
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _state;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _state = state;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _evenFlags;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _evenFlags = (evenFlags);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _accessNumber = (accessNumber);
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     
                  public:
                     /**
                      * Generated
                      */
                     Cell();
                     
                     /**
                      * Generated
                      */
                     Cell(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                     
                     /**
                      * Generated
                      */
                     virtual ~Cell();
                     
                     
                     inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._cellDescriptionIndex;
                     }
                     
                     
                     
                     inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                     }
                     
                     
                     
                     inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._isInside;
                     }
                     
                     
                     
                     inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._isInside = isInside;
                     }
                     
                     
                     
                     inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._state;
                     }
                     
                     
                     
                     inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._state = state;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._evenFlags;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._evenFlags = (evenFlags);
                     }
                     
                     
                     
                     inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        return _persistentRecords._evenFlags[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._evenFlags[elementIndex]= evenFlags;
                        
                     }
                     
                     
                     
                     inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._evenFlags.flip(elementIndex);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._accessNumber;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._accessNumber = (accessNumber);
                     }
                     
                     
                     
                     inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        return _persistentRecords._accessNumber[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                        _persistentRecords._accessNumber[elementIndex]= accessNumber;
                        
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const State& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getStateMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     CellPacked convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifndef DaStGenPackedPadding
                    #define DaStGenPackedPadding 1      // 32 bit version
                    // #define DaStGenPackedPadding 2   // 64 bit version
                  #endif
                  
                  
                  #ifdef PackedRecords
                     #pragma pack (push, DaStGenPackedPadding)
                  #endif
                  
                  /**
                   * @author This class is generated by DaStGen
                   * 		   DataStructureGenerator (DaStGen)
                   * 		   2007-2009 Wolfgang Eckhardt
                   * 		   2012      Tobias Weinzierl
                   *
                   * 		   build date: 12-04-2013 09:18
                   *
                   * @date   31/07/2013 16:41
                   */
                  class peanoclaw::records::CellPacked { 
                     
                     public:
                        
                        typedef peanoclaw::records::Cell::State State;
                        
                        struct PersistentRecords {
                           int _cellDescriptionIndex;
                           tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                           
                           /** mapping of records:
                           || Member 	|| startbit 	|| length
                            |  isInside	| startbit 0	| #bits 1
                            |  state	| startbit 1	| #bits 2
                            |  evenFlags	| startbit 3	| #bits DIMENSIONS
                            */
                           short int _packedRecords0;
                           
                           /**
                            * Generated
                            */
                           PersistentRecords();
                           
                           /**
                            * Generated
                            */
                           PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                           
                           
                           inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _cellDescriptionIndex;
                           }
                           
                           
                           
                           inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _cellDescriptionIndex = cellDescriptionIndex;
                           }
                           
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              short int tmp = static_cast<short int>(_packedRecords0 & mask);
                              tmp = static_cast<short int>(tmp >> (3));
                              std::bitset<DIMENSIONS> result = tmp;
                              return result;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                              _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _accessNumber = (accessNumber);
                           }
                           
                           
                           
                        };
                        
                     private: 
                        PersistentRecords _persistentRecords;
                        
                     public:
                        /**
                         * Generated
                         */
                        CellPacked();
                        
                        /**
                         * Generated
                         */
                        CellPacked(const PersistentRecords& persistentRecords);
                        
                        /**
                         * Generated
                         */
                        CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber);
                        
                        /**
                         * Generated
                         */
                        virtual ~CellPacked();
                        
                        
                        inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._cellDescriptionIndex;
                        }
                        
                        
                        
                        inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                        }
                        
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                           tmp = static_cast<short int>(tmp >> (3));
                           std::bitset<DIMENSIONS> result = tmp;
                           return result;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                           mask = static_cast<short int>(mask << (3));
                           _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                           _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                        }
                        
                        
                        
                        inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           int mask = 1 << (3);
                           mask = mask << elementIndex;
                           return (_persistentRecords._packedRecords0& mask);
                        }
                        
                        
                        
                        inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           assertion(!evenFlags || evenFlags==1);
                           int shift        = 3 + elementIndex; 
                           int mask         = 1     << (shift);
                           int shiftedValue = evenFlags << (shift);
                           _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                           _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                        }
                        
                        
                        
                        inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           int mask = 1 << (3);
                           mask = mask << elementIndex;
                           _persistentRecords._packedRecords0^= mask;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           return _persistentRecords._accessNumber[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           _persistentRecords._accessNumber[elementIndex]= accessNumber;
                           
                        }
                        
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const State& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getStateMapping();
                        
                        /**
                         * Generated
                         */
                        std::string toString() const;
                        
                        /**
                         * Generated
                         */
                        void toString(std::ostream& out) const;
                        
                        
                        PersistentRecords getPersistentRecords() const;
                        /**
                         * Generated
                         */
                        Cell convert() const;
                        
                        
                     #ifdef Parallel
                        protected:
                           static tarch::logging::Log _log;
                           
                           int _senderDestinationRank;
                           
                        public:
                           
                           /**
                            * Global that represents the mpi datatype.
                            * There are two variants: Datatype identifies only those attributes marked with
                            * parallelise. FullDatatype instead identifies the whole record with all fields.
                            */
                           static MPI_Datatype Datatype;
                           static MPI_Datatype FullDatatype;
                           
                           /**
                            * Initializes the data type for the mpi operations. Has to be called
                            * before the very first send or receive operation is called.
                            */
                           static void initDatatype();
                           
                           static void shutdownDatatype();
                           
                           void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           int getSenderRank() const;
                           
                     #endif
                        
                     };
                     
                     #ifdef PackedRecords
                     #pragma pack (pop)
                     #endif
                     
                     
                     
                  
               #elif defined(Parallel) && defined(SharedMemoryParallelisation) && defined(Debug)
                  /**
                   * @author This class is generated by DaStGen
                   * 		   DataStructureGenerator (DaStGen)
                   * 		   2007-2009 Wolfgang Eckhardt
                   * 		   2012      Tobias Weinzierl
                   *
                   * 		   build date: 12-04-2013 09:18
                   *
                   * @date   31/07/2013 16:41
                   */
                  class peanoclaw::records::Cell { 
                     
                     public:
                        
                        typedef peanoclaw::records::CellPacked Packed;
                        
                        enum State {
                           Leaf = 0, Refined = 1, Root = 2
                        };
                        
                        struct PersistentRecords {
                           int _cellDescriptionIndex;
                           bool _isInside;
                           State _state;
                           int _level;
                           #ifdef UseManualAlignment
                           std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                           #else
                           std::bitset<DIMENSIONS> _evenFlags;
                           #endif
                           #ifdef UseManualAlignment
                           tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                           #else
                           tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                           #endif
                           int _responsibleRank;
                           bool _subtreeHoldsWorker;
                           double _nodeWorkload;
                           double _localWorkload;
                           double _totalWorkload;
                           bool _cellIsAForkCandidate;
                           int _numberOfLoadsFromInputStream;
                           int _numberOfStoresToOutputStream;
                           /**
                            * Generated
                            */
                           PersistentRecords();
                           
                           /**
                            * Generated
                            */
                           PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                           
                           
                           inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _cellDescriptionIndex;
                           }
                           
                           
                           
                           inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _cellDescriptionIndex = cellDescriptionIndex;
                           }
                           
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _isInside;
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _isInside = isInside;
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _state;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _state = state;
                           }
                           
                           
                           
                           inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _level;
                           }
                           
                           
                           
                           inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _level = level;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _evenFlags;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _evenFlags = (evenFlags);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _accessNumber = (accessNumber);
                           }
                           
                           
                           
                           inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _responsibleRank;
                           }
                           
                           
                           
                           inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _responsibleRank = responsibleRank;
                           }
                           
                           
                           
                           inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _subtreeHoldsWorker = subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _nodeWorkload;
                           }
                           
                           
                           
                           inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _nodeWorkload = nodeWorkload;
                           }
                           
                           
                           
                           inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _localWorkload;
                           }
                           
                           
                           
                           inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _localWorkload = localWorkload;
                           }
                           
                           
                           
                           inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _totalWorkload;
                           }
                           
                           
                           
                           inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _totalWorkload = totalWorkload;
                           }
                           
                           
                           
                           inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _cellIsAForkCandidate;
                           }
                           
                           
                           
                           inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _cellIsAForkCandidate = cellIsAForkCandidate;
                           }
                           
                           
                           
                           inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _numberOfLoadsFromInputStream;
                           }
                           
                           
                           
                           inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                           }
                           
                           
                           
                           inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _numberOfStoresToOutputStream;
                           }
                           
                           
                           
                           inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                           }
                           
                           
                           
                        };
                        
                     private: 
                        PersistentRecords _persistentRecords;
                        
                     public:
                        /**
                         * Generated
                         */
                        Cell();
                        
                        /**
                         * Generated
                         */
                        Cell(const PersistentRecords& persistentRecords);
                        
                        /**
                         * Generated
                         */
                        Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                        
                        /**
                         * Generated
                         */
                        virtual ~Cell();
                        
                        
                        inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._cellDescriptionIndex;
                        }
                        
                        
                        
                        inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                        }
                        
                        
                        
                        inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._isInside;
                        }
                        
                        
                        
                        inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._isInside = isInside;
                        }
                        
                        
                        
                        inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._state;
                        }
                        
                        
                        
                        inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._state = state;
                        }
                        
                        
                        
                        inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._level;
                        }
                        
                        
                        
                        inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._level = level;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._evenFlags;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._evenFlags = (evenFlags);
                        }
                        
                        
                        
                        inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           return _persistentRecords._evenFlags[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           _persistentRecords._evenFlags[elementIndex]= evenFlags;
                           
                        }
                        
                        
                        
                        inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           _persistentRecords._evenFlags.flip(elementIndex);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._accessNumber;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._accessNumber = (accessNumber);
                        }
                        
                        
                        
                        inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           return _persistentRecords._accessNumber[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                           _persistentRecords._accessNumber[elementIndex]= accessNumber;
                           
                        }
                        
                        
                        
                        inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._responsibleRank;
                        }
                        
                        
                        
                        inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._responsibleRank = responsibleRank;
                        }
                        
                        
                        
                        inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                        }
                        
                        
                        
                        inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._nodeWorkload;
                        }
                        
                        
                        
                        inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._nodeWorkload = nodeWorkload;
                        }
                        
                        
                        
                        inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._localWorkload;
                        }
                        
                        
                        
                        inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._localWorkload = localWorkload;
                        }
                        
                        
                        
                        inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._totalWorkload;
                        }
                        
                        
                        
                        inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._totalWorkload = totalWorkload;
                        }
                        
                        
                        
                        inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._cellIsAForkCandidate;
                        }
                        
                        
                        
                        inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
                        }
                        
                        
                        
                        inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                        }
                        
                        
                        
                        inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._numberOfStoresToOutputStream;
                        }
                        
                        
                        
                        inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                        }
                        
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const State& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getStateMapping();
                        
                        /**
                         * Generated
                         */
                        std::string toString() const;
                        
                        /**
                         * Generated
                         */
                        void toString(std::ostream& out) const;
                        
                        
                        PersistentRecords getPersistentRecords() const;
                        /**
                         * Generated
                         */
                        CellPacked convert() const;
                        
                        
                     #ifdef Parallel
                        protected:
                           static tarch::logging::Log _log;
                           
                           int _senderDestinationRank;
                           
                        public:
                           
                           /**
                            * Global that represents the mpi datatype.
                            * There are two variants: Datatype identifies only those attributes marked with
                            * parallelise. FullDatatype instead identifies the whole record with all fields.
                            */
                           static MPI_Datatype Datatype;
                           static MPI_Datatype FullDatatype;
                           
                           /**
                            * Initializes the data type for the mpi operations. Has to be called
                            * before the very first send or receive operation is called.
                            */
                           static void initDatatype();
                           
                           static void shutdownDatatype();
                           
                           void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           int getSenderRank() const;
                           
                     #endif
                        
                     };
                     
                     #ifndef DaStGenPackedPadding
                       #define DaStGenPackedPadding 1      // 32 bit version
                       // #define DaStGenPackedPadding 2   // 64 bit version
                     #endif
                     
                     
                     #ifdef PackedRecords
                        #pragma pack (push, DaStGenPackedPadding)
                     #endif
                     
                     /**
                      * @author This class is generated by DaStGen
                      * 		   DataStructureGenerator (DaStGen)
                      * 		   2007-2009 Wolfgang Eckhardt
                      * 		   2012      Tobias Weinzierl
                      *
                      * 		   build date: 12-04-2013 09:18
                      *
                      * @date   31/07/2013 16:41
                      */
                     class peanoclaw::records::CellPacked { 
                        
                        public:
                           
                           typedef peanoclaw::records::Cell::State State;
                           
                           struct PersistentRecords {
                              int _cellDescriptionIndex;
                              int _level;
                              tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                              int _responsibleRank;
                              bool _subtreeHoldsWorker;
                              double _nodeWorkload;
                              double _localWorkload;
                              double _totalWorkload;
                              int _numberOfLoadsFromInputStream;
                              int _numberOfStoresToOutputStream;
                              
                              /** mapping of records:
                              || Member 	|| startbit 	|| length
                               |  isInside	| startbit 0	| #bits 1
                               |  state	| startbit 1	| #bits 2
                               |  evenFlags	| startbit 3	| #bits DIMENSIONS
                               |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                               */
                              short int _packedRecords0;
                              
                              /**
                               * Generated
                               */
                              PersistentRecords();
                              
                              /**
                               * Generated
                               */
                              PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                              
                              
                              inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _cellDescriptionIndex;
                              }
                              
                              
                              
                              inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _cellDescriptionIndex = cellDescriptionIndex;
                              }
                              
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                              }
                              
                              
                              
                              inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _level;
                              }
                              
                              
                              
                              inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _level = level;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                 tmp = static_cast<short int>(tmp >> (3));
                                 std::bitset<DIMENSIONS> result = tmp;
                                 return result;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                 _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _responsibleRank;
                              }
                              
                              
                              
                              inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _responsibleRank = responsibleRank;
                              }
                              
                              
                              
                              inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _subtreeHoldsWorker = subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _nodeWorkload;
                              }
                              
                              
                              
                              inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _nodeWorkload = nodeWorkload;
                              }
                              
                              
                              
                              inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _localWorkload;
                              }
                              
                              
                              
                              inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _localWorkload = localWorkload;
                              }
                              
                              
                              
                              inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _totalWorkload;
                              }
                              
                              
                              
                              inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _totalWorkload = totalWorkload;
                              }
                              
                              
                              
                              inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                              }
                              
                              
                              
                              inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _numberOfLoadsFromInputStream;
                              }
                              
                              
                              
                              inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                              }
                              
                              
                              
                              inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _numberOfStoresToOutputStream;
                              }
                              
                              
                              
                              inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                              }
                              
                              
                              
                           };
                           
                        private: 
                           PersistentRecords _persistentRecords;
                           
                        public:
                           /**
                            * Generated
                            */
                           CellPacked();
                           
                           /**
                            * Generated
                            */
                           CellPacked(const PersistentRecords& persistentRecords);
                           
                           /**
                            * Generated
                            */
                           CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                           
                           /**
                            * Generated
                            */
                           virtual ~CellPacked();
                           
                           
                           inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._cellDescriptionIndex;
                           }
                           
                           
                           
                           inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                           }
                           
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                           }
                           
                           
                           
                           inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._level;
                           }
                           
                           
                           
                           inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._level = level;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                              tmp = static_cast<short int>(tmp >> (3));
                              std::bitset<DIMENSIONS> result = tmp;
                              return result;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                              mask = static_cast<short int>(mask << (3));
                              _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                              _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                           }
                           
                           
                           
                           inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              int mask = 1 << (3);
                              mask = mask << elementIndex;
                              return (_persistentRecords._packedRecords0& mask);
                           }
                           
                           
                           
                           inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              assertion(!evenFlags || evenFlags==1);
                              int shift        = 3 + elementIndex; 
                              int mask         = 1     << (shift);
                              int shiftedValue = evenFlags << (shift);
                              _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                              _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                           }
                           
                           
                           
                           inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              int mask = 1 << (3);
                              mask = mask << elementIndex;
                              _persistentRecords._packedRecords0^= mask;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._accessNumber = (accessNumber);
                           }
                           
                           
                           
                           inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._accessNumber[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._accessNumber[elementIndex]= accessNumber;
                              
                           }
                           
                           
                           
                           inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._responsibleRank;
                           }
                           
                           
                           
                           inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._responsibleRank = responsibleRank;
                           }
                           
                           
                           
                           inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._nodeWorkload;
                           }
                           
                           
                           
                           inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._nodeWorkload = nodeWorkload;
                           }
                           
                           
                           
                           inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._localWorkload;
                           }
                           
                           
                           
                           inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._localWorkload = localWorkload;
                           }
                           
                           
                           
                           inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._totalWorkload;
                           }
                           
                           
                           
                           inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._totalWorkload = totalWorkload;
                           }
                           
                           
                           
                           inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._numberOfLoadsFromInputStream;
                           }
                           
                           
                           
                           inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                           }
                           
                           
                           
                           inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._numberOfStoresToOutputStream;
                           }
                           
                           
                           
                           inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                           }
                           
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const State& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getStateMapping();
                           
                           /**
                            * Generated
                            */
                           std::string toString() const;
                           
                           /**
                            * Generated
                            */
                           void toString(std::ostream& out) const;
                           
                           
                           PersistentRecords getPersistentRecords() const;
                           /**
                            * Generated
                            */
                           Cell convert() const;
                           
                           
                        #ifdef Parallel
                           protected:
                              static tarch::logging::Log _log;
                              
                              int _senderDestinationRank;
                              
                           public:
                              
                              /**
                               * Global that represents the mpi datatype.
                               * There are two variants: Datatype identifies only those attributes marked with
                               * parallelise. FullDatatype instead identifies the whole record with all fields.
                               */
                              static MPI_Datatype Datatype;
                              static MPI_Datatype FullDatatype;
                              
                              /**
                               * Initializes the data type for the mpi operations. Has to be called
                               * before the very first send or receive operation is called.
                               */
                              static void initDatatype();
                              
                              static void shutdownDatatype();
                              
                              void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              int getSenderRank() const;
                              
                        #endif
                           
                        };
                        
                        #ifdef PackedRecords
                        #pragma pack (pop)
                        #endif
                        
                        
                        
                     
                  #elif defined(Parallel) && defined(Debug) && !defined(SharedMemoryParallelisation)
                     /**
                      * @author This class is generated by DaStGen
                      * 		   DataStructureGenerator (DaStGen)
                      * 		   2007-2009 Wolfgang Eckhardt
                      * 		   2012      Tobias Weinzierl
                      *
                      * 		   build date: 12-04-2013 09:18
                      *
                      * @date   31/07/2013 16:41
                      */
                     class peanoclaw::records::Cell { 
                        
                        public:
                           
                           typedef peanoclaw::records::CellPacked Packed;
                           
                           enum State {
                              Leaf = 0, Refined = 1, Root = 2
                           };
                           
                           struct PersistentRecords {
                              int _cellDescriptionIndex;
                              bool _isInside;
                              State _state;
                              int _level;
                              #ifdef UseManualAlignment
                              std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                              #else
                              std::bitset<DIMENSIONS> _evenFlags;
                              #endif
                              #ifdef UseManualAlignment
                              tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                              #else
                              tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                              #endif
                              int _responsibleRank;
                              bool _subtreeHoldsWorker;
                              double _nodeWorkload;
                              double _localWorkload;
                              double _totalWorkload;
                              bool _cellIsAForkCandidate;
                              /**
                               * Generated
                               */
                              PersistentRecords();
                              
                              /**
                               * Generated
                               */
                              PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                              
                              
                              inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _cellDescriptionIndex;
                              }
                              
                              
                              
                              inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _cellDescriptionIndex = cellDescriptionIndex;
                              }
                              
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _isInside;
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _isInside = isInside;
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _state;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _state = state;
                              }
                              
                              
                              
                              inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _level;
                              }
                              
                              
                              
                              inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _level = level;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _evenFlags;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _evenFlags = (evenFlags);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _responsibleRank;
                              }
                              
                              
                              
                              inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _responsibleRank = responsibleRank;
                              }
                              
                              
                              
                              inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _subtreeHoldsWorker = subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _nodeWorkload;
                              }
                              
                              
                              
                              inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _nodeWorkload = nodeWorkload;
                              }
                              
                              
                              
                              inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _localWorkload;
                              }
                              
                              
                              
                              inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _localWorkload = localWorkload;
                              }
                              
                              
                              
                              inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _totalWorkload;
                              }
                              
                              
                              
                              inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _totalWorkload = totalWorkload;
                              }
                              
                              
                              
                              inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _cellIsAForkCandidate;
                              }
                              
                              
                              
                              inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _cellIsAForkCandidate = cellIsAForkCandidate;
                              }
                              
                              
                              
                           };
                           
                        private: 
                           PersistentRecords _persistentRecords;
                           
                        public:
                           /**
                            * Generated
                            */
                           Cell();
                           
                           /**
                            * Generated
                            */
                           Cell(const PersistentRecords& persistentRecords);
                           
                           /**
                            * Generated
                            */
                           Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                           
                           /**
                            * Generated
                            */
                           virtual ~Cell();
                           
                           
                           inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._cellDescriptionIndex;
                           }
                           
                           
                           
                           inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                           }
                           
                           
                           
                           inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._isInside;
                           }
                           
                           
                           
                           inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._isInside = isInside;
                           }
                           
                           
                           
                           inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._state;
                           }
                           
                           
                           
                           inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._state = state;
                           }
                           
                           
                           
                           inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._level;
                           }
                           
                           
                           
                           inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._level = level;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._evenFlags;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._evenFlags = (evenFlags);
                           }
                           
                           
                           
                           inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              return _persistentRecords._evenFlags[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._evenFlags[elementIndex]= evenFlags;
                              
                           }
                           
                           
                           
                           inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS);
                              _persistentRecords._evenFlags.flip(elementIndex);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._accessNumber;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._accessNumber = (accessNumber);
                           }
                           
                           
                           
                           inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              return _persistentRecords._accessNumber[elementIndex];
                              
                           }
                           
                           
                           
                           inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion(elementIndex>=0);
                              assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                              _persistentRecords._accessNumber[elementIndex]= accessNumber;
                              
                           }
                           
                           
                           
                           inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._responsibleRank;
                           }
                           
                           
                           
                           inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._responsibleRank = responsibleRank;
                           }
                           
                           
                           
                           inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                           }
                           
                           
                           
                           inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._nodeWorkload;
                           }
                           
                           
                           
                           inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._nodeWorkload = nodeWorkload;
                           }
                           
                           
                           
                           inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._localWorkload;
                           }
                           
                           
                           
                           inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._localWorkload = localWorkload;
                           }
                           
                           
                           
                           inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._totalWorkload;
                           }
                           
                           
                           
                           inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._totalWorkload = totalWorkload;
                           }
                           
                           
                           
                           inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _persistentRecords._cellIsAForkCandidate;
                           }
                           
                           
                           
                           inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
                           }
                           
                           
                           /**
                            * Generated
                            */
                           static std::string toString(const State& param);
                           
                           /**
                            * Generated
                            */
                           static std::string getStateMapping();
                           
                           /**
                            * Generated
                            */
                           std::string toString() const;
                           
                           /**
                            * Generated
                            */
                           void toString(std::ostream& out) const;
                           
                           
                           PersistentRecords getPersistentRecords() const;
                           /**
                            * Generated
                            */
                           CellPacked convert() const;
                           
                           
                        #ifdef Parallel
                           protected:
                              static tarch::logging::Log _log;
                              
                              int _senderDestinationRank;
                              
                           public:
                              
                              /**
                               * Global that represents the mpi datatype.
                               * There are two variants: Datatype identifies only those attributes marked with
                               * parallelise. FullDatatype instead identifies the whole record with all fields.
                               */
                              static MPI_Datatype Datatype;
                              static MPI_Datatype FullDatatype;
                              
                              /**
                               * Initializes the data type for the mpi operations. Has to be called
                               * before the very first send or receive operation is called.
                               */
                              static void initDatatype();
                              
                              static void shutdownDatatype();
                              
                              void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                              
                              int getSenderRank() const;
                              
                        #endif
                           
                        };
                        
                        #ifndef DaStGenPackedPadding
                          #define DaStGenPackedPadding 1      // 32 bit version
                          // #define DaStGenPackedPadding 2   // 64 bit version
                        #endif
                        
                        
                        #ifdef PackedRecords
                           #pragma pack (push, DaStGenPackedPadding)
                        #endif
                        
                        /**
                         * @author This class is generated by DaStGen
                         * 		   DataStructureGenerator (DaStGen)
                         * 		   2007-2009 Wolfgang Eckhardt
                         * 		   2012      Tobias Weinzierl
                         *
                         * 		   build date: 12-04-2013 09:18
                         *
                         * @date   31/07/2013 16:41
                         */
                        class peanoclaw::records::CellPacked { 
                           
                           public:
                              
                              typedef peanoclaw::records::Cell::State State;
                              
                              struct PersistentRecords {
                                 int _cellDescriptionIndex;
                                 int _level;
                                 tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                 int _responsibleRank;
                                 bool _subtreeHoldsWorker;
                                 double _nodeWorkload;
                                 double _localWorkload;
                                 double _totalWorkload;
                                 
                                 /** mapping of records:
                                 || Member 	|| startbit 	|| length
                                  |  isInside	| startbit 0	| #bits 1
                                  |  state	| startbit 1	| #bits 2
                                  |  evenFlags	| startbit 3	| #bits DIMENSIONS
                                  |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                                  */
                                 short int _packedRecords0;
                                 
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords();
                                 
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                                 
                                 
                                 inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _cellDescriptionIndex = cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                                 }
                                 
                                 
                                 
                                 inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _level;
                                 }
                                 
                                 
                                 
                                 inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _level = level;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                    tmp = static_cast<short int>(tmp >> (3));
                                    std::bitset<DIMENSIONS> result = tmp;
                                    return result;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                    _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _responsibleRank = responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _subtreeHoldsWorker = subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _nodeWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _nodeWorkload = nodeWorkload;
                                 }
                                 
                                 
                                 
                                 inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _localWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _localWorkload = localWorkload;
                                 }
                                 
                                 
                                 
                                 inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _totalWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _totalWorkload = totalWorkload;
                                 }
                                 
                                 
                                 
                                 inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                              };
                              
                           private: 
                              PersistentRecords _persistentRecords;
                              
                           public:
                              /**
                               * Generated
                               */
                              CellPacked();
                              
                              /**
                               * Generated
                               */
                              CellPacked(const PersistentRecords& persistentRecords);
                              
                              /**
                               * Generated
                               */
                              CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate);
                              
                              /**
                               * Generated
                               */
                              virtual ~CellPacked();
                              
                              
                              inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._cellDescriptionIndex;
                              }
                              
                              
                              
                              inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                              }
                              
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                              }
                              
                              
                              
                              inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._level;
                              }
                              
                              
                              
                              inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._level = level;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                                 tmp = static_cast<short int>(tmp >> (3));
                                 std::bitset<DIMENSIONS> result = tmp;
                                 return result;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                 mask = static_cast<short int>(mask << (3));
                                 _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                                 _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                              }
                              
                              
                              
                              inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 int mask = 1 << (3);
                                 mask = mask << elementIndex;
                                 return (_persistentRecords._packedRecords0& mask);
                              }
                              
                              
                              
                              inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 assertion(!evenFlags || evenFlags==1);
                                 int shift        = 3 + elementIndex; 
                                 int mask         = 1     << (shift);
                                 int shiftedValue = evenFlags << (shift);
                                 _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                                 _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                              }
                              
                              
                              
                              inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 int mask = 1 << (3);
                                 mask = mask << elementIndex;
                                 _persistentRecords._packedRecords0^= mask;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 return _persistentRecords._accessNumber[elementIndex];
                                 
                              }
                              
                              
                              
                              inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                 
                              }
                              
                              
                              
                              inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._responsibleRank;
                              }
                              
                              
                              
                              inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._responsibleRank = responsibleRank;
                              }
                              
                              
                              
                              inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._nodeWorkload;
                              }
                              
                              
                              
                              inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._nodeWorkload = nodeWorkload;
                              }
                              
                              
                              
                              inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._localWorkload;
                              }
                              
                              
                              
                              inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._localWorkload = localWorkload;
                              }
                              
                              
                              
                              inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._totalWorkload;
                              }
                              
                              
                              
                              inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._totalWorkload = totalWorkload;
                              }
                              
                              
                              
                              inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                              }
                              
                              
                              
                              inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                              }
                              
                              
                              /**
                               * Generated
                               */
                              static std::string toString(const State& param);
                              
                              /**
                               * Generated
                               */
                              static std::string getStateMapping();
                              
                              /**
                               * Generated
                               */
                              std::string toString() const;
                              
                              /**
                               * Generated
                               */
                              void toString(std::ostream& out) const;
                              
                              
                              PersistentRecords getPersistentRecords() const;
                              /**
                               * Generated
                               */
                              Cell convert() const;
                              
                              
                           #ifdef Parallel
                              protected:
                                 static tarch::logging::Log _log;
                                 
                                 int _senderDestinationRank;
                                 
                              public:
                                 
                                 /**
                                  * Global that represents the mpi datatype.
                                  * There are two variants: Datatype identifies only those attributes marked with
                                  * parallelise. FullDatatype instead identifies the whole record with all fields.
                                  */
                                 static MPI_Datatype Datatype;
                                 static MPI_Datatype FullDatatype;
                                 
                                 /**
                                  * Initializes the data type for the mpi operations. Has to be called
                                  * before the very first send or receive operation is called.
                                  */
                                 static void initDatatype();
                                 
                                 static void shutdownDatatype();
                                 
                                 void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 int getSenderRank() const;
                                 
                           #endif
                              
                           };
                           
                           #ifdef PackedRecords
                           #pragma pack (pop)
                           #endif
                           
                           
                           
                        
                     #elif defined(Parallel) && !defined(Debug) && defined(SharedMemoryParallelisation)
                        /**
                         * @author This class is generated by DaStGen
                         * 		   DataStructureGenerator (DaStGen)
                         * 		   2007-2009 Wolfgang Eckhardt
                         * 		   2012      Tobias Weinzierl
                         *
                         * 		   build date: 12-04-2013 09:18
                         *
                         * @date   31/07/2013 16:41
                         */
                        class peanoclaw::records::Cell { 
                           
                           public:
                              
                              typedef peanoclaw::records::CellPacked Packed;
                              
                              enum State {
                                 Leaf = 0, Refined = 1, Root = 2
                              };
                              
                              struct PersistentRecords {
                                 int _cellDescriptionIndex;
                                 bool _isInside;
                                 State _state;
                                 #ifdef UseManualAlignment
                                 std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                                 #else
                                 std::bitset<DIMENSIONS> _evenFlags;
                                 #endif
                                 #ifdef UseManualAlignment
                                 tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                                 #else
                                 tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                 #endif
                                 int _responsibleRank;
                                 bool _subtreeHoldsWorker;
                                 double _nodeWorkload;
                                 double _localWorkload;
                                 double _totalWorkload;
                                 bool _cellIsAForkCandidate;
                                 int _numberOfLoadsFromInputStream;
                                 int _numberOfStoresToOutputStream;
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords();
                                 
                                 /**
                                  * Generated
                                  */
                                 PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                 
                                 
                                 inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _cellDescriptionIndex = cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _isInside;
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _isInside = isInside;
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _state;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _state = state;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _evenFlags;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _evenFlags = (evenFlags);
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _responsibleRank = responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _subtreeHoldsWorker = subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _nodeWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _nodeWorkload = nodeWorkload;
                                 }
                                 
                                 
                                 
                                 inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _localWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _localWorkload = localWorkload;
                                 }
                                 
                                 
                                 
                                 inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _totalWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _totalWorkload = totalWorkload;
                                 }
                                 
                                 
                                 
                                 inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _cellIsAForkCandidate;
                                 }
                                 
                                 
                                 
                                 inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _cellIsAForkCandidate = cellIsAForkCandidate;
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 
                              };
                              
                           private: 
                              PersistentRecords _persistentRecords;
                              
                           public:
                              /**
                               * Generated
                               */
                              Cell();
                              
                              /**
                               * Generated
                               */
                              Cell(const PersistentRecords& persistentRecords);
                              
                              /**
                               * Generated
                               */
                              Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                              
                              /**
                               * Generated
                               */
                              virtual ~Cell();
                              
                              
                              inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._cellDescriptionIndex;
                              }
                              
                              
                              
                              inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                              }
                              
                              
                              
                              inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._isInside;
                              }
                              
                              
                              
                              inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._isInside = isInside;
                              }
                              
                              
                              
                              inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._state;
                              }
                              
                              
                              
                              inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._state = state;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._evenFlags;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._evenFlags = (evenFlags);
                              }
                              
                              
                              
                              inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 return _persistentRecords._evenFlags[elementIndex];
                                 
                              }
                              
                              
                              
                              inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 _persistentRecords._evenFlags[elementIndex]= evenFlags;
                                 
                              }
                              
                              
                              
                              inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS);
                                 _persistentRecords._evenFlags.flip(elementIndex);
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._accessNumber;
                              }
                              
                              
                              
                              /**
                               * Generated and optimized
                               * 
                               * If you realise a for loop using exclusively arrays (vectors) and compile 
                               * with -DUseManualAlignment you may add 
                               * \code
                               #pragma vector aligned
                               #pragma simd
                               \endcode to this for loop to enforce your compiler to use SSE/AVX.
                               * 
                               * The alignment is tied to the unpacked records, i.e. for packed class
                               * variants the machine's natural alignment is switched off to recude the  
                               * memory footprint. Do not use any SSE/AVX operations or 
                               * vectorisation on the result for the packed variants, as the data is misaligned. 
                               * If you rely on vectorisation, convert the underlying record 
                               * into the unpacked version first. 
                               * 
                               * @see convert()
                               */
                              inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._accessNumber = (accessNumber);
                              }
                              
                              
                              
                              inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 return _persistentRecords._accessNumber[elementIndex];
                                 
                              }
                              
                              
                              
                              inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 assertion(elementIndex>=0);
                                 assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                 _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                 
                              }
                              
                              
                              
                              inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._responsibleRank;
                              }
                              
                              
                              
                              inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._responsibleRank = responsibleRank;
                              }
                              
                              
                              
                              inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                              }
                              
                              
                              
                              inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._nodeWorkload;
                              }
                              
                              
                              
                              inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._nodeWorkload = nodeWorkload;
                              }
                              
                              
                              
                              inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._localWorkload;
                              }
                              
                              
                              
                              inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._localWorkload = localWorkload;
                              }
                              
                              
                              
                              inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._totalWorkload;
                              }
                              
                              
                              
                              inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._totalWorkload = totalWorkload;
                              }
                              
                              
                              
                              inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._cellIsAForkCandidate;
                              }
                              
                              
                              
                              inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._cellIsAForkCandidate = cellIsAForkCandidate;
                              }
                              
                              
                              
                              inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._numberOfLoadsFromInputStream;
                              }
                              
                              
                              
                              inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                              }
                              
                              
                              
                              inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 return _persistentRecords._numberOfStoresToOutputStream;
                              }
                              
                              
                              
                              inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                 _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                              }
                              
                              
                              /**
                               * Generated
                               */
                              static std::string toString(const State& param);
                              
                              /**
                               * Generated
                               */
                              static std::string getStateMapping();
                              
                              /**
                               * Generated
                               */
                              std::string toString() const;
                              
                              /**
                               * Generated
                               */
                              void toString(std::ostream& out) const;
                              
                              
                              PersistentRecords getPersistentRecords() const;
                              /**
                               * Generated
                               */
                              CellPacked convert() const;
                              
                              
                           #ifdef Parallel
                              protected:
                                 static tarch::logging::Log _log;
                                 
                                 int _senderDestinationRank;
                                 
                              public:
                                 
                                 /**
                                  * Global that represents the mpi datatype.
                                  * There are two variants: Datatype identifies only those attributes marked with
                                  * parallelise. FullDatatype instead identifies the whole record with all fields.
                                  */
                                 static MPI_Datatype Datatype;
                                 static MPI_Datatype FullDatatype;
                                 
                                 /**
                                  * Initializes the data type for the mpi operations. Has to be called
                                  * before the very first send or receive operation is called.
                                  */
                                 static void initDatatype();
                                 
                                 static void shutdownDatatype();
                                 
                                 void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                 
                                 int getSenderRank() const;
                                 
                           #endif
                              
                           };
                           
                           #ifndef DaStGenPackedPadding
                             #define DaStGenPackedPadding 1      // 32 bit version
                             // #define DaStGenPackedPadding 2   // 64 bit version
                           #endif
                           
                           
                           #ifdef PackedRecords
                              #pragma pack (push, DaStGenPackedPadding)
                           #endif
                           
                           /**
                            * @author This class is generated by DaStGen
                            * 		   DataStructureGenerator (DaStGen)
                            * 		   2007-2009 Wolfgang Eckhardt
                            * 		   2012      Tobias Weinzierl
                            *
                            * 		   build date: 12-04-2013 09:18
                            *
                            * @date   31/07/2013 16:41
                            */
                           class peanoclaw::records::CellPacked { 
                              
                              public:
                                 
                                 typedef peanoclaw::records::Cell::State State;
                                 
                                 struct PersistentRecords {
                                    int _cellDescriptionIndex;
                                    tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                    int _responsibleRank;
                                    bool _subtreeHoldsWorker;
                                    double _nodeWorkload;
                                    double _localWorkload;
                                    double _totalWorkload;
                                    int _numberOfLoadsFromInputStream;
                                    int _numberOfStoresToOutputStream;
                                    
                                    /** mapping of records:
                                    || Member 	|| startbit 	|| length
                                     |  isInside	| startbit 0	| #bits 1
                                     |  state	| startbit 1	| #bits 2
                                     |  evenFlags	| startbit 3	| #bits DIMENSIONS
                                     |  cellIsAForkCandidate	| startbit DIMENSIONS + 3	| #bits 1
                                     */
                                    short int _packedRecords0;
                                    
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords();
                                    
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                    
                                    
                                    inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _cellDescriptionIndex;
                                    }
                                    
                                    
                                    
                                    inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _cellDescriptionIndex = cellDescriptionIndex;
                                    }
                                    
                                    
                                    
                                    inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                    }
                                    
                                    
                                    
                                    inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                    }
                                    
                                    
                                    
                                    inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                       tmp = static_cast<short int>(tmp >> (3));
                                       std::bitset<DIMENSIONS> result = tmp;
                                       return result;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                       _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _accessNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _accessNumber = (accessNumber);
                                    }
                                    
                                    
                                    
                                    inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _responsibleRank;
                                    }
                                    
                                    
                                    
                                    inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _responsibleRank = responsibleRank;
                                    }
                                    
                                    
                                    
                                    inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _subtreeHoldsWorker;
                                    }
                                    
                                    
                                    
                                    inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _subtreeHoldsWorker = subtreeHoldsWorker;
                                    }
                                    
                                    
                                    
                                    inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _nodeWorkload;
                                    }
                                    
                                    
                                    
                                    inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _nodeWorkload = nodeWorkload;
                                    }
                                    
                                    
                                    
                                    inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _localWorkload;
                                    }
                                    
                                    
                                    
                                    inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _localWorkload = localWorkload;
                                    }
                                    
                                    
                                    
                                    inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _totalWorkload;
                                    }
                                    
                                    
                                    
                                    inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _totalWorkload = totalWorkload;
                                    }
                                    
                                    
                                    
                                    inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                    }
                                    
                                    
                                    
                                    inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (DIMENSIONS + 3);
   _packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                 };
                                 
                              private: 
                                 PersistentRecords _persistentRecords;
                                 
                              public:
                                 /**
                                  * Generated
                                  */
                                 CellPacked();
                                 
                                 /**
                                  * Generated
                                  */
                                 CellPacked(const PersistentRecords& persistentRecords);
                                 
                                 /**
                                  * Generated
                                  */
                                 CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& responsibleRank, const bool& subtreeHoldsWorker, const double& nodeWorkload, const double& localWorkload, const double& totalWorkload, const bool& cellIsAForkCandidate, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                 
                                 /**
                                  * Generated
                                  */
                                 virtual ~CellPacked();
                                 
                                 
                                 inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                                    tmp = static_cast<short int>(tmp >> (3));
                                    std::bitset<DIMENSIONS> result = tmp;
                                    return result;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                    mask = static_cast<short int>(mask << (3));
                                    _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                                    _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                                 }
                                 
                                 
                                 
                                 inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    int mask = 1 << (3);
                                    mask = mask << elementIndex;
                                    return (_persistentRecords._packedRecords0& mask);
                                 }
                                 
                                 
                                 
                                 inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    assertion(!evenFlags || evenFlags==1);
                                    int shift        = 3 + elementIndex; 
                                    int mask         = 1     << (shift);
                                    int shiftedValue = evenFlags << (shift);
                                    _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                                    _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                                 }
                                 
                                 
                                 
                                 inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    int mask = 1 << (3);
                                    mask = mask << elementIndex;
                                    _persistentRecords._packedRecords0^= mask;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    return _persistentRecords._accessNumber[elementIndex];
                                    
                                 }
                                 
                                 
                                 
                                 inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                    
                                 }
                                 
                                 
                                 
                                 inline int getResponsibleRank() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline void setResponsibleRank(const int& responsibleRank) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._responsibleRank = responsibleRank;
                                 }
                                 
                                 
                                 
                                 inline bool getSubtreeHoldsWorker() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline void setSubtreeHoldsWorker(const bool& subtreeHoldsWorker) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._subtreeHoldsWorker = subtreeHoldsWorker;
                                 }
                                 
                                 
                                 
                                 inline double getNodeWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._nodeWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setNodeWorkload(const double& nodeWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._nodeWorkload = nodeWorkload;
                                 }
                                 
                                 
                                 
                                 inline double getLocalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._localWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setLocalWorkload(const double& localWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._localWorkload = localWorkload;
                                 }
                                 
                                 
                                 
                                 inline double getTotalWorkload() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._totalWorkload;
                                 }
                                 
                                 
                                 
                                 inline void setTotalWorkload(const double& totalWorkload) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._totalWorkload = totalWorkload;
                                 }
                                 
                                 
                                 
                                 inline bool getCellIsAForkCandidate() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (DIMENSIONS + 3);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                                 }
                                 
                                 
                                 
                                 inline void setCellIsAForkCandidate(const bool& cellIsAForkCandidate) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    short int mask = 1 << (DIMENSIONS + 3);
   _persistentRecords._packedRecords0 = static_cast<short int>( cellIsAForkCandidate ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string toString(const State& param);
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string getStateMapping();
                                 
                                 /**
                                  * Generated
                                  */
                                 std::string toString() const;
                                 
                                 /**
                                  * Generated
                                  */
                                 void toString(std::ostream& out) const;
                                 
                                 
                                 PersistentRecords getPersistentRecords() const;
                                 /**
                                  * Generated
                                  */
                                 Cell convert() const;
                                 
                                 
                              #ifdef Parallel
                                 protected:
                                    static tarch::logging::Log _log;
                                    
                                    int _senderDestinationRank;
                                    
                                 public:
                                    
                                    /**
                                     * Global that represents the mpi datatype.
                                     * There are two variants: Datatype identifies only those attributes marked with
                                     * parallelise. FullDatatype instead identifies the whole record with all fields.
                                     */
                                    static MPI_Datatype Datatype;
                                    static MPI_Datatype FullDatatype;
                                    
                                    /**
                                     * Initializes the data type for the mpi operations. Has to be called
                                     * before the very first send or receive operation is called.
                                     */
                                    static void initDatatype();
                                    
                                    static void shutdownDatatype();
                                    
                                    void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    int getSenderRank() const;
                                    
                              #endif
                                 
                              };
                              
                              #ifdef PackedRecords
                              #pragma pack (pop)
                              #endif
                              
                              
                              
                           
                        #elif !defined(Parallel) && defined(SharedMemoryParallelisation) && defined(Debug)
                           /**
                            * @author This class is generated by DaStGen
                            * 		   DataStructureGenerator (DaStGen)
                            * 		   2007-2009 Wolfgang Eckhardt
                            * 		   2012      Tobias Weinzierl
                            *
                            * 		   build date: 12-04-2013 09:18
                            *
                            * @date   31/07/2013 16:41
                            */
                           class peanoclaw::records::Cell { 
                              
                              public:
                                 
                                 typedef peanoclaw::records::CellPacked Packed;
                                 
                                 enum State {
                                    Leaf = 0, Refined = 1, Root = 2
                                 };
                                 
                                 struct PersistentRecords {
                                    int _cellDescriptionIndex;
                                    bool _isInside;
                                    State _state;
                                    int _level;
                                    #ifdef UseManualAlignment
                                    std::bitset<DIMENSIONS> _evenFlags __attribute__((aligned(VectorisationAlignment)));
                                    #else
                                    std::bitset<DIMENSIONS> _evenFlags;
                                    #endif
                                    #ifdef UseManualAlignment
                                    tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber __attribute__((aligned(VectorisationAlignment)));
                                    #else
                                    tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                    #endif
                                    int _numberOfLoadsFromInputStream;
                                    int _numberOfStoresToOutputStream;
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords();
                                    
                                    /**
                                     * Generated
                                     */
                                    PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                    
                                    
                                    inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _cellDescriptionIndex;
                                    }
                                    
                                    
                                    
                                    inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _cellDescriptionIndex = cellDescriptionIndex;
                                    }
                                    
                                    
                                    
                                    inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _isInside;
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _isInside = isInside;
                                    }
                                    
                                    
                                    
                                    inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _state;
                                    }
                                    
                                    
                                    
                                    inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _state = state;
                                    }
                                    
                                    
                                    
                                    inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _level;
                                    }
                                    
                                    
                                    
                                    inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _level = level;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _evenFlags;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _evenFlags = (evenFlags);
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _accessNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _accessNumber = (accessNumber);
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                 };
                                 
                              private: 
                                 PersistentRecords _persistentRecords;
                                 
                              public:
                                 /**
                                  * Generated
                                  */
                                 Cell();
                                 
                                 /**
                                  * Generated
                                  */
                                 Cell(const PersistentRecords& persistentRecords);
                                 
                                 /**
                                  * Generated
                                  */
                                 Cell(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                 
                                 /**
                                  * Generated
                                  */
                                 virtual ~Cell();
                                 
                                 
                                 inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                                 }
                                 
                                 
                                 
                                 inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._isInside;
                                 }
                                 
                                 
                                 
                                 inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._isInside = isInside;
                                 }
                                 
                                 
                                 
                                 inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._state;
                                 }
                                 
                                 
                                 
                                 inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._state = state;
                                 }
                                 
                                 
                                 
                                 inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._level;
                                 }
                                 
                                 
                                 
                                 inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._level = level;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._evenFlags;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._evenFlags = (evenFlags);
                                 }
                                 
                                 
                                 
                                 inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    return _persistentRecords._evenFlags[elementIndex];
                                    
                                 }
                                 
                                 
                                 
                                 inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    _persistentRecords._evenFlags[elementIndex]= evenFlags;
                                    
                                 }
                                 
                                 
                                 
                                 inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS);
                                    _persistentRecords._evenFlags.flip(elementIndex);
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._accessNumber;
                                 }
                                 
                                 
                                 
                                 /**
                                  * Generated and optimized
                                  * 
                                  * If you realise a for loop using exclusively arrays (vectors) and compile 
                                  * with -DUseManualAlignment you may add 
                                  * \code
                                  #pragma vector aligned
                                  #pragma simd
                                  \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                  * 
                                  * The alignment is tied to the unpacked records, i.e. for packed class
                                  * variants the machine's natural alignment is switched off to recude the  
                                  * memory footprint. Do not use any SSE/AVX operations or 
                                  * vectorisation on the result for the packed variants, as the data is misaligned. 
                                  * If you rely on vectorisation, convert the underlying record 
                                  * into the unpacked version first. 
                                  * 
                                  * @see convert()
                                  */
                                 inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._accessNumber = (accessNumber);
                                 }
                                 
                                 
                                 
                                 inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    return _persistentRecords._accessNumber[elementIndex];
                                    
                                 }
                                 
                                 
                                 
                                 inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    assertion(elementIndex>=0);
                                    assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                    _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                    
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                 }
                                 
                                 
                                 
                                 inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    return _persistentRecords._numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 
                                 inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                    _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                 }
                                 
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string toString(const State& param);
                                 
                                 /**
                                  * Generated
                                  */
                                 static std::string getStateMapping();
                                 
                                 /**
                                  * Generated
                                  */
                                 std::string toString() const;
                                 
                                 /**
                                  * Generated
                                  */
                                 void toString(std::ostream& out) const;
                                 
                                 
                                 PersistentRecords getPersistentRecords() const;
                                 /**
                                  * Generated
                                  */
                                 CellPacked convert() const;
                                 
                                 
                              #ifdef Parallel
                                 protected:
                                    static tarch::logging::Log _log;
                                    
                                    int _senderDestinationRank;
                                    
                                 public:
                                    
                                    /**
                                     * Global that represents the mpi datatype.
                                     * There are two variants: Datatype identifies only those attributes marked with
                                     * parallelise. FullDatatype instead identifies the whole record with all fields.
                                     */
                                    static MPI_Datatype Datatype;
                                    static MPI_Datatype FullDatatype;
                                    
                                    /**
                                     * Initializes the data type for the mpi operations. Has to be called
                                     * before the very first send or receive operation is called.
                                     */
                                    static void initDatatype();
                                    
                                    static void shutdownDatatype();
                                    
                                    void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                    
                                    int getSenderRank() const;
                                    
                              #endif
                                 
                              };
                              
                              #ifndef DaStGenPackedPadding
                                #define DaStGenPackedPadding 1      // 32 bit version
                                // #define DaStGenPackedPadding 2   // 64 bit version
                              #endif
                              
                              
                              #ifdef PackedRecords
                                 #pragma pack (push, DaStGenPackedPadding)
                              #endif
                              
                              /**
                               * @author This class is generated by DaStGen
                               * 		   DataStructureGenerator (DaStGen)
                               * 		   2007-2009 Wolfgang Eckhardt
                               * 		   2012      Tobias Weinzierl
                               *
                               * 		   build date: 12-04-2013 09:18
                               *
                               * @date   31/07/2013 16:41
                               */
                              class peanoclaw::records::CellPacked { 
                                 
                                 public:
                                    
                                    typedef peanoclaw::records::Cell::State State;
                                    
                                    struct PersistentRecords {
                                       int _cellDescriptionIndex;
                                       int _level;
                                       tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> _accessNumber;
                                       int _numberOfLoadsFromInputStream;
                                       int _numberOfStoresToOutputStream;
                                       
                                       /** mapping of records:
                                       || Member 	|| startbit 	|| length
                                        |  isInside	| startbit 0	| #bits 1
                                        |  state	| startbit 1	| #bits 2
                                        |  evenFlags	| startbit 3	| #bits DIMENSIONS
                                        */
                                       short int _packedRecords0;
                                       
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords();
                                       
                                       /**
                                        * Generated
                                        */
                                       PersistentRecords(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                       
                                       
                                       inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _cellDescriptionIndex;
                                       }
                                       
                                       
                                       
                                       inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _cellDescriptionIndex = cellDescriptionIndex;
                                       }
                                       
                                       
                                       
                                       inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                                       }
                                       
                                       
                                       
                                       inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isInside ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                                       }
                                       
                                       
                                       
                                       inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                       }
                                       
                                       
                                       
                                       inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | state << (1));
                                       }
                                       
                                       
                                       
                                       inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _level;
                                       }
                                       
                                       
                                       
                                       inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _level = level;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                          mask = static_cast<short int>(mask << (3));
                                          short int tmp = static_cast<short int>(_packedRecords0 & mask);
                                          tmp = static_cast<short int>(tmp >> (3));
                                          std::bitset<DIMENSIONS> result = tmp;
                                          return result;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                          mask = static_cast<short int>(mask << (3));
                                          _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
                                          _packedRecords0 = static_cast<short int>(_packedRecords0 | evenFlags.to_ulong() << (3));
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _accessNumber;
                                       }
                                       
                                       
                                       
                                       /**
                                        * Generated and optimized
                                        * 
                                        * If you realise a for loop using exclusively arrays (vectors) and compile 
                                        * with -DUseManualAlignment you may add 
                                        * \code
                                        #pragma vector aligned
                                        #pragma simd
                                        \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                        * 
                                        * The alignment is tied to the unpacked records, i.e. for packed class
                                        * variants the machine's natural alignment is switched off to recude the  
                                        * memory footprint. Do not use any SSE/AVX operations or 
                                        * vectorisation on the result for the packed variants, as the data is misaligned. 
                                        * If you rely on vectorisation, convert the underlying record 
                                        * into the unpacked version first. 
                                        * 
                                        * @see convert()
                                        */
                                       inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _accessNumber = (accessNumber);
                                       }
                                       
                                       
                                       
                                       inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _numberOfLoadsFromInputStream;
                                       }
                                       
                                       
                                       
                                       inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                       }
                                       
                                       
                                       
                                       inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          return _numberOfStoresToOutputStream;
                                       }
                                       
                                       
                                       
                                       inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                          _numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                       }
                                       
                                       
                                       
                                    };
                                    
                                 private: 
                                    PersistentRecords _persistentRecords;
                                    
                                 public:
                                    /**
                                     * Generated
                                     */
                                    CellPacked();
                                    
                                    /**
                                     * Generated
                                     */
                                    CellPacked(const PersistentRecords& persistentRecords);
                                    
                                    /**
                                     * Generated
                                     */
                                    CellPacked(const int& cellDescriptionIndex, const bool& isInside, const State& state, const int& level, const std::bitset<DIMENSIONS>& evenFlags, const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber, const int& numberOfLoadsFromInputStream, const int& numberOfStoresToOutputStream);
                                    
                                    /**
                                     * Generated
                                     */
                                    virtual ~CellPacked();
                                    
                                    
                                    inline int getCellDescriptionIndex() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._cellDescriptionIndex;
                                    }
                                    
                                    
                                    
                                    inline void setCellDescriptionIndex(const int& cellDescriptionIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._cellDescriptionIndex = cellDescriptionIndex;
                                    }
                                    
                                    
                                    
                                    inline bool getIsInside() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                                    }
                                    
                                    
                                    
                                    inline void setIsInside(const bool& isInside) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isInside ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                                    }
                                    
                                    
                                    
                                    inline State getState() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (State) tmp;
                                    }
                                    
                                    
                                    
                                    inline void setState(const State& state) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion((state >= 0 && state <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | state << (1));
                                    }
                                    
                                    
                                    
                                    inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._level;
                                    }
                                    
                                    
                                    
                                    inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._level = level;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline std::bitset<DIMENSIONS> getEvenFlags() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
                                       tmp = static_cast<short int>(tmp >> (3));
                                       std::bitset<DIMENSIONS> result = tmp;
                                       return result;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setEvenFlags(const std::bitset<DIMENSIONS>& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       short int mask = (short int) (1 << (DIMENSIONS)) - 1 ;
                                       mask = static_cast<short int>(mask << (3));
                                       _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
                                       _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | evenFlags.to_ulong() << (3));
                                    }
                                    
                                    
                                    
                                    inline bool getEvenFlags(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       int mask = 1 << (3);
                                       mask = mask << elementIndex;
                                       return (_persistentRecords._packedRecords0& mask);
                                    }
                                    
                                    
                                    
                                    inline void setEvenFlags(int elementIndex, const bool& evenFlags) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       assertion(!evenFlags || evenFlags==1);
                                       int shift        = 3 + elementIndex; 
                                       int mask         = 1     << (shift);
                                       int shiftedValue = evenFlags << (shift);
                                       _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 & ~mask;
                                       _persistentRecords._packedRecords0 = _persistentRecords._packedRecords0 |  shiftedValue;
                                    }
                                    
                                    
                                    
                                    inline void flipEvenFlags(int elementIndex) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS);
                                       int mask = 1 << (3);
                                       mask = mask << elementIndex;
                                       _persistentRecords._packedRecords0^= mask;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int> getAccessNumber() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._accessNumber;
                                    }
                                    
                                    
                                    
                                    /**
                                     * Generated and optimized
                                     * 
                                     * If you realise a for loop using exclusively arrays (vectors) and compile 
                                     * with -DUseManualAlignment you may add 
                                     * \code
                                     #pragma vector aligned
                                     #pragma simd
                                     \endcode to this for loop to enforce your compiler to use SSE/AVX.
                                     * 
                                     * The alignment is tied to the unpacked records, i.e. for packed class
                                     * variants the machine's natural alignment is switched off to recude the  
                                     * memory footprint. Do not use any SSE/AVX operations or 
                                     * vectorisation on the result for the packed variants, as the data is misaligned. 
                                     * If you rely on vectorisation, convert the underlying record 
                                     * into the unpacked version first. 
                                     * 
                                     * @see convert()
                                     */
                                    inline void setAccessNumber(const tarch::la::Vector<DIMENSIONS_TIMES_TWO,short int>& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._accessNumber = (accessNumber);
                                    }
                                    
                                    
                                    
                                    inline short int getAccessNumber(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       return _persistentRecords._accessNumber[elementIndex];
                                       
                                    }
                                    
                                    
                                    
                                    inline void setAccessNumber(int elementIndex, const short int& accessNumber) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       assertion(elementIndex>=0);
                                       assertion(elementIndex<DIMENSIONS_TIMES_TWO);
                                       _persistentRecords._accessNumber[elementIndex]= accessNumber;
                                       
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfLoadsFromInputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfLoadsFromInputStream(const int& numberOfLoadsFromInputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._numberOfLoadsFromInputStream = numberOfLoadsFromInputStream;
                                    }
                                    
                                    
                                    
                                    inline int getNumberOfStoresToOutputStream() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       return _persistentRecords._numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    
                                    inline void setNumberOfStoresToOutputStream(const int& numberOfStoresToOutputStream) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                                       _persistentRecords._numberOfStoresToOutputStream = numberOfStoresToOutputStream;
                                    }
                                    
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string toString(const State& param);
                                    
                                    /**
                                     * Generated
                                     */
                                    static std::string getStateMapping();
                                    
                                    /**
                                     * Generated
                                     */
                                    std::string toString() const;
                                    
                                    /**
                                     * Generated
                                     */
                                    void toString(std::ostream& out) const;
                                    
                                    
                                    PersistentRecords getPersistentRecords() const;
                                    /**
                                     * Generated
                                     */
                                    Cell convert() const;
                                    
                                    
                                 #ifdef Parallel
                                    protected:
                                       static tarch::logging::Log _log;
                                       
                                       int _senderDestinationRank;
                                       
                                    public:
                                       
                                       /**
                                        * Global that represents the mpi datatype.
                                        * There are two variants: Datatype identifies only those attributes marked with
                                        * parallelise. FullDatatype instead identifies the whole record with all fields.
                                        */
                                       static MPI_Datatype Datatype;
                                       static MPI_Datatype FullDatatype;
                                       
                                       /**
                                        * Initializes the data type for the mpi operations. Has to be called
                                        * before the very first send or receive operation is called.
                                        */
                                       static void initDatatype();
                                       
                                       static void shutdownDatatype();
                                       
                                       void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                       
                                       void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                       
                                       static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                                       
                                       int getSenderRank() const;
                                       
                                 #endif
                                    
                                 };
                                 
                                 #ifdef PackedRecords
                                 #pragma pack (pop)
                                 #endif
                                 
                                 
                                 
                              
                           #endif
                           
                           #endif
                           
